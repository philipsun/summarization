 Image description is a new natural language generation task, where the aim is to generate a human-like description of an image. The evaluation of computer-generated text is a notoriously difficult problem, however, the quality of image descriptions has typically been measured using unigram bleu and human judgements. The focus of this paper is to determine the correlation of automatic measures with human judgements for this task. We estimate the correlation of unigram and Smoothed bleu , ter , rouge-su4 , and Meteor against human judgements on two data sets. The main finding is that unigram bleu has a weak correlation, and Meteor has the strongest correlation with human judgements. An older woman with a small dog in the snow. A woman and a cat are outside in the snow. A woman in a brown vest is walking on the snow with an animal. A woman with a red scarf covering her head walks with her cat on snow-covered ground. Heavy set woman in snow with a cat.