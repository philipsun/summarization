 We have introduced a novel generalized language model as the systematic combination of skip n -grams and modified Kneser-Ney smoothing. The main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result. Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization. In an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases. The relative improvement in perplexity is up to 12.7 u'\u2062' % for large data sets. GLMs also performs particularly well on small and sparse sets of training data. On a very small training data set we observed a reduction of perplexity by 25.7 u'\u2062' % . Our experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data.