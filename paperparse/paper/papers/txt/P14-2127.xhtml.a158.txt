 Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT [ 21 ] significantly outperforms mainstream methods that only train on small tuning sets. However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit. To address this problem, we extend \newcite yu+:2013 to syntax-based MT by generalizing their latent variable u'\u201c' violation-fixing u'\u201d' perceptron from graphs to hypergraphs. Experiments confirm that our method leads to up to +1.2 Bleu improvement over mainstream methods such as Mert and Pro . 1.5em 1em algorithmAlgorithm