#-------------------------------------------------------------------------------
# Name:        sxpPaperIntroSentLabel
# Purpose:
#
# Author:      sunxp
#
# Created:     26/05/2015
# Copyright:   (c) sunxp 2015
# Licence:     <your licence>
#-------------------------------------------------------------------------------
intro_sent = [['P14-1007',
    r'Recently, there has been increased community interest in the theoretical and practical analysis of what \citeAMor:Dae:12 call modality and negation, i.e. linguistic expressions that modulate the certainty or factuality of propositions',
    r'Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining [9] or sentiment analysis [5]',
    r'Owing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Conference on Computational Language Learning (CoNLL)',
    r'In this work, we return to the 2012 *SEM task from a deliberately semantics-centered point of view, focusing on the hardest of the three sub-problems: scope resolution',
    r'22Resolving negation scope is a more difficult sub-problem at least in part because (unlike cue and event identification) it is concerned with much larger, non-local and often discontinuous parts of each utterance',
    r'Our contributions are three-fold',
    r'Theoretically, we correlate the structures at play in the \citeAMor:Dae:12 view on negation with formal semantic analyses',
    r'methodologically, we demonstrate how to approach the task in terms of underspecified, logical-form semantics',
    r'and practically, our combined system retroactively ‘wins’ the 2012 *SEM Shared Task'
    ],

    ['P14-1008',
    r'Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees ',
    r'It is expressive enough to represent complex natural language queries on a relational database, yet simple enough to be latently learned from question-answer pairs.',
    r' In this paper, we equip DCS with logical inference, which, in one point of view, is “the best way of testing an NLP system’s semantic capacity”'
    r'It should be noted that, however, a framework primarily designed for question answering is not readily suited for logical inference',
    r'Because, answers returned by a query depend on the specific database, but implication is independent of any databases. ',
    r'Thus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database',
    r'An inference engine is built to handle inference on abstract denotations.',
    r'Moreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation [], to propose a way of generating knowledge in logical representation from entailment rules [], which are by now typically considered as syntactic rewriting rules.',
    r'We test our system on FraCaS [] and PASCAL RTE datasets [].'
    r'The experiments show: (i) a competitive performance on FraCaS dataset;',
    r'(ii) a big impact of our automatically generated on-the-fly knowledge in achieving high recall for a logic-based RTE system;',
    r'and (iii) a result that outperforms state-of-the-art RTE system on RTE5 data.'
    ],
    [ 'p14-1010',
	r'Morphological segmentation is considered to be indispensable when translating between English and morphologically complex languages such as Arabic',
	r'Morphological complexity leads to much higher type to token ratios than English, which can create sparsity problems during translation model estimation',
	r'Morphological segmentation addresses this issue by splitting surface forms into meaningful morphemes, while also performing orthographic transformations to further reduce sparsity',
	r'Desegmentation is typically performed as a post-processing step that is independent from the decoding process',
	r'In this work, we show that it is possible to maintain the sparsity-reducing benefit of segmentation while translating directly into unsegmented text',
	r'We desegment a large set of possible decoder outputs by processing -best lists or lattices, which allows us to consider both the segmented and desegmented output before locking in the decoder  decision'
	],
	['p14-1011',
	r'Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment , translation confidence estimation , phrase reordering prediction , translation modelling and language modelling ',
	r'Most of these works attempt to improve some components in SMT based on , which converts a word into a dense, low dimensional, real-valued vector representation ',
	r'The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules',
	r'Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work',
	r'In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector',
	r'Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that our phrase embedding can fully represent the phrase and best fit the phrase-based SMT',
	r'The core idea behind is that a phrase and its correct translation should share the same semantic meaning',
	r'Similarly, non-translation pairs should have different semantic meanings, and this information can also be used to guide learning semantic phrase embeddings',
	r'In our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs',
	r'The experiments show that up to 72% of the phrase table can be discarded without significant decrease on the translation quality, and in decoding with phrasal semantic similarities up to 1.7 BLEU score improvement over the state-of-the-art baseline can be achieved'
	],
	['p14-1012',
	r'Recently, many new features have been explored for SMT and significant performance have been obtained in terms of translation quality, such as syntactic features, sparse features, and reordering features',
	r'Instead of designing new features based on intuition, linguistic knowledge and domain, for the first time, Maskey and Zhou (2012) explored the possibility of inducing new features in an unsupervised fashion using deep belief net (DBN) for hierarchical phrase-based translation model',
	r'However, the above approach has two major shortcomings',
	r'In this paper, we strive to effectively address the above two shortcomings, and systematically explore the possibility of learning new features using deep (multi-layer) neural networks (DNN, which is usually referred under the name ) for SMT',
	r'By using the input data as the teacher, the semi-supervised fine-tuning process of DAE addresses the problem of back-propagation without a teacher, which makes the DAE learn more powerful and abstract features ',
	r'Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition for DAEs (HCDAE) that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs , which shows further improvement compared with single DAE in our experiments',
	r'Thus, instead of GMM, we use DNN (DBN, DAE and HCDAE) to learn new non-parametric features, which has the similar evolution in speech recognition ',
	r'Finally, we conduct large-scale experiments on IWSLT and NIST Chinese-English translation tasks, respectively, and the results demonstrate that our solutions solve the two aforementioned shortcomings successfully',
	r'Our semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning'
	],
	['p14-1013',
	r'Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems',
	r'Although these methods are effective and proven successful in many SMT systems, they only leverage within-sentence contexts which are insufficient in exploring broader information',
	r'Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance',
	r'Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents',
	r'Attempts on topic-based translation modeling include topic-specific lexicon translation models , topic similarity models for synchronous rules , and document-level translation with topic coherence ',
	r'Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents',
	r'Attempts on topic-based translation modeling include topic-specific lexicon translation models , topic similarity models for synchronous rules , and document-level translation with topic coherence ',
	r'However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries',
	r'In this paper, we propose a novel approach to learning topic representations for sentences',
	r'Since the information within the sentence is insufficient for topic modeling, we first enrich sentence contexts via Information Retrieval (IR) methods using content words in the sentence as queries, so that topic-related monolingual documents can be collected',
	r'These topic-related documents are utilized to learn a specific topic representation for each sentence using a neural network based approach',
	r'Topic-related rules are selected according to distributional similarity with the source text, which helps hypotheses generation in SMT decoding',
	r'We integrate topic similarity features in the log-linear model and evaluate the performance on the NIST Chinese-to-English translation task',
	r'Experimental results demonstrate that our model significantly improves translation accuracy over a state-of-the-art baseline'
	],
	['p14-1014',
	r'Exploiting useful information from the web data can be the key to improving web domain tagging',
	r'Our approach consists of two phrases',
	r'In the pre-training phase, we learn an encoder that converts the web text into an intermediate representation, which acts as useful features for prediction tasks',
	r'In the fine-tuning phase, the parameters of the network are optimized on a set of labelled training data using guided learning',
	r'The learned model preserves the property of preferring to tag words first',
	r'While most previous work focus on in-domain sequential labelling or cross-domain classification tasks, we are the first to learn representations for web-domain structured prediction',
	r'In this work, we investigate both strategies and give empirical comparisons in the cross-domain setting',
	r'Our results suggest that while both strategies improve in-domain tagging accuracies, keeping the learned representation unchanged consistently results in better cross-domain accuracies',
	r'Our method achieves a average accuracy across the web-domain, which is the best result reported so far on this data set, higher than those given by ensembled syntactic parsers'
	],
	['p14-1015',
	r'Mining problem-solution pairs from discussion forums has attracted much attention from the scholarly community in the recent past',
	r'In this paper, we address the problem of unsupervised solution post identification from discussion forums',
	r'Among the first papers to address the solution identification problem was the unsupervised approach proposed by ',
	r'It employs a graph propagation method that prioritizes posts that are (a) more similar to the problem post, (b) more similar to other posts, and (c) authored by a more authoritative user, to be labeled as solution posts',
	r'Though seen to be effective in identifying solutions from travel forums, the first two assumptions, (a) and (b), were seen to be not very reliable in solution identification in other kinds of discussion boards',
	r'A variety of high precision assumptions such as , , , , and so on have been seen to be useful in solution identification',
	r'We propose an unsupervised method for solution identificatio',
	r'We model the lexical correlation and solution post character using regularized translation models and unigram language models respectively',
	r'In particular, we show that by using post position as the only non-textual feature, we are able to achieve accuracies comparable to supervision-based approaches that use many structural features '
	],
	['p14-1018',
	r'The existing batch models for predicting latent user attributes rely on thousands of tweets per author ',
	r'The existing batch models for predicting latent user attributes rely on thousands of tweets per author ',
	r'In this paper we analyze and go beyond static models formulating personal analytics in social media as a streaming task',
	r'We first evaluate batch models that are cognizant of low-resource prediction setting described above, maximizing the efficiency of content in calculating personal analytics',
	r'In addition, we propose streaming models for personal analytics that dynamically update user labels based on their stream of communications which has been addressed previously by',
	r'Such models better capture the real-time nature of evidence being used in latent author attribute predictions tasks'
	],
	['p14-1019',
	r'Dependency parsing is commonly cast as a maximization problem over a parameterized scoring function.',
	r'Much of the recent work on parsing has been focused on improving methods for solving the combinatorial maximization inference problems.',
	r'We depart from this view and instead focus on using highly expressive scoring functions with substantially simpler inference procedures',
	r'In this paper, we introduce a sampling-based parser that places few or no constraints on the scoring function.',
	r'Because the inference procedure is so simple, it is important that the parameters of the scoring function are chosen in a manner that facilitates how we climb the scoring function in small steps.',
	r'Here we apply SampleRank to parsing, applying several modifications such as the proposal distributions mentioned earlier.',
	r'The benefits of sampling-based learning go beyond stand-alone parsing',
	r'Our joint parsing-tagging model provides an alternative to the widely-adopted pipeline setup.',
	r'We evaluate our method on benchmark multilingual dependency corpora.'
	],
	['p14-1020',
	r'Because NLP models typically treat sentences independently, NLP problems have long been seen as “embarrassingly parallel” – large corpora can be processed arbitrarily fast by simply sending different sentences to different machines.',
	r'However, recent trends in computer architecture, particularly the development of powerful “general purpose” GPUs, have changed the landscape even for problems that parallelize at the sentence level',
	r'The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on.',
	r'Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide.',
	r'In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-to-fine pruning to a GPU setting',
	r'However, in this paper, we present a system that finds a middle ground, where some level of sparsity can be maintained without losing the parallelism of the GPU.',
	r'To that end, we extend our coarse-to-fine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass.',
	r'We then implement minimum-Bayes-risk parsing via the max recall algorithm of Goodman (1996).'
	],
	['p14-1022',
	r'Naive context-free grammars, such as those embodied by standard treebank annotations, do not parse well because their symbols have too little context to constrain their syntactic behavior. ',
	r'Much of the last few decades of parsing research has therefore focused on propagating contextual information from the leaves of the tree to internal nodes. ',
	r'There have been non-local approaches as well, such as tree-substitution parsers [2, 26], neural net parsers [13], and rerankers [6, 4, 14]. ',
	r'In this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features',
	r'By contrast, we investigate the extent to which we need a grammar at all.',
	r'Our parser is also able to generalize well across languages with little tuning: it achieves state-of-the-art results on multilingual parsing, scoring higher than the best single-parser system from the SPMRL 2013 Shared Task on a range of languages, as well as on the competition’s average F1 metric.',
	r'Our parser can be easily adapted to this task by replacing the X-bar grammar over treebank symbols with a grammar over the sentiment values to encode the output variables and then adding n-gram indicators to our feature set to capture the bulk of the lexical effects.',
	r'When applied to this task, our system generally matches their accuracy overall and is able to outperform it on the overall sentence-level subtask.'
	],
	['p14-1023',
	r'A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions',
	r'It has been clear for decades now that raw co-occurrence counts don’t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques.',
	r'The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus.',
	r'This new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defined supervised learning step.',
	r'At the same time, supervision comes at no manual annotation cost, given that the context windows used for training can be automatically extracted from an unannotated corpus (indeed, they are the very same data used to build traditional DSMs).',
	r'Surprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks [1, 5, 10, 11, 41, 37], the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs.',
	r'Whatever the reasons, we know of just three works reporting direct comparisons, all limited in their scope.',
	r'In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks. '
	],
	['p14-1024',
	r'They argue that metaphors play a fundamental communicative role in verbal and written interactions, claiming that much of our everyday language is delivered in metaphorical terms.',
	r'Given the prevalence and importance of metaphoric language, effective automatic detection of metaphors would have a number of benefits, both practical and scientific.',
	r'Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation.',
	r'However, metaphor detection is a hard problem',
	r'Previous work has focused on metaphor identification in English, using both extensive manually-created linguistic resources and corpus-based approaches.',
	r'Our work makes the following contributions',
	r'we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses',
	r'we create new metaphor-annotated corpora for Russian and English ',
	r'using a paradigm of model transfer, we provide support for the hypothesis that metaphors are conceptual (rather than lexical) in nature by showing that our English-trained model can detect metaphors in Spanish, Farsi, and Russian.'
	],
	['p14-1025',
	r'The automatic determination of word sense information has been a long-term pursuit of the NLP community',
	r'Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly high-accuracy back-off heuristic for word sense disambiguation (wsd) is to tag each instance of a given word with its predominant sense',
	r'Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required [12].',
	r'In this paper, we propose a method which uses topic models to estimate word sense distributions.',
	r'Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus.',
	r'Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. ',
	r'We further propose an application of our proposed method to the identification of such novel senses. '
	],
	['p14-1027',
	r'Over the past two decades psychologists have investigated the role that function words might play in human language acquisition.',
	r'The goal of this paper is to determine whether computational models of human language acquisition can provide support for the hypothesis that function words are treated specially in human language acquisition.',
	r'We show that a model equipped with the ability to learn some rudimentary properties of the target language’s function words is able to learn the vocabulary of that language more accurately than a model that is identical except that it is incapable of learning these generalisations about function words.',
	r'This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition. ',
	r'As a reviewer points out, the changes we make to our models to incorporate function words can be viewed as “building in” substantive information about possible human languages.'
	],
	['p14-1028',
	r'Therefore, word segmentation is a preliminary and important pre-process for Chinese language processing.',
	r'Most previous systems address this problem by treating this task as a sequence labeling problem where each character is assigned a tag indicating its position in the word.',
	r'However, the ability of these models is restricted by the design of features and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus.',
	r'Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering.',
	r'Workable as previous neural network models seem, a limitation of them to be pointed out is that the tag-tag interaction, tag-character interaction and character-character interaction are not well modeled.',
	r'In previous neural network models, however, hardly can such interactional effects be fully captured relying only on the simple transition score and the single non-linear transformation (See section 2).',
	r'In order to address this problem, we propose a new model called Max-Margin Tensor Neural Network (MMTNN) that explicitly models the interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation. ',
	r' Therefore, we integrate additional simple character bigram features into our model and the result shows that our model can achieve a competitive performance that other systems hardly achieve unless they use more complex task-specific features.'
	],
	['p14-1029',
	r'Understanding the impact of negation on sentiment is essential in automatic analysis of sentiment. ',
	r'In this paper, we refer to a negation word as the negator (e.g., isn’t), a text span being modified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn’t very good) as the negated phrase.',
	r'The recently available Stanford Sentiment Treebank [] renders manually annotated, real-valued sentiment scores for all phrases in parse trees.',
	r'This paper describes a quantitative study of the effect of a list of frequent negators on sentiment.',
	r'We regard the negators’ behavior as an underlying function embedded in annotated data; we aim to model this function from different aspects.',
	r'By examining sentiment compositions of negators and arguments, we model the quantitative behavior of negators in changing sentiment.'
	],
	['p14-1031',
	r'Accordingly, extracting sentiment at the fine-grained level (e.g. at the sentence- or phrase-level) has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks ',
	r'In this paper, we focus on the task of sentence-level sentiment classification in online reviews',
	r'The importance of discourse for sentiment analysis has become increasingly recognized.',
	r'Very little work has explored long-distance discourse relations for sentiment analysis.',
	r'Obtaining sentiment labels at the fine-grained level is costly. ',
	r'In this paper, we propose a sentence-level sentiment classification method that can (1) incorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning. ',
	r'Our work is the first to explore PR for sentiment analysis.',
	r'Unlike most previous work, we explore a rich set of structural constraints that cannot be naturally encoded in the feature-label form, and show that such constraints can improve the performance of the CRF model.',
	r'Experimental results show that our model outperforms state-of-the-art methods in both the supervised and semi-supervised settings. ',
	r'We also show that discourse knowledge is highly useful for improving sentence-level sentiment classification.'
	],
	['p14-1033',
	r'Aspect extraction has two subtasks: aspect term extraction and aspect term resolution.',
	r'Recently, topic models have been extensively applied to aspect extraction because they can perform both subtasks at the same time while other existing methods all need two separate steps',
	r'To tackle the problem, several semi-supervised topic models, also called knowledge-based topic models, have been proposed. ',
	r'We mine the prior knowledge directly from a large amount of relevant data without any user intervention, and thus make this approach fully automatic. ',
	r'There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps.',
	r'We thus propose to first use LDA to learn topics/aspects from each individual domain and then discover the shared aspects (or topics) and aspect terms among a subset of domains. ',
	r'We propose a method to solve this problem, which also results in a new topic model, called AKL (Automated Knowledge LDA), whose inference can exploit the automatically learned prior knowledge and handle the issues of incorrect knowledge to produce superior aspects.'
	],
	['p14-1039',
	r'Rajkumar & White [28, 36] have recently shown that some rather egregious surface realization errors—in the sense that the reader would likely end up with the wrong interpretation—can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model [35], as reviewed in the next section.',
	r'As Neumann & van Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse each realization to see how many interpretations it has, keeping only those that have a single reading; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output. ',
	r'In this paper, we investigate whether Neumann & van Noord’s brute-force strategy for avoiding ambiguities in surface realization can be updated to only avoid vicious ambiguities, extending (and revising) Van Deemter’s general strategy to all kinds of structural ambiguity, not just the one investigated by Khan et al. ',
	r'A potential obstacle, of course, is that automatic parsers may not be sufficiently representative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length, even with carefully edited news text',
	r'Our simple reranking strategy for self-monitoring is to rerank the realizer’s n-best list by parse accuracy, preserving the original order in case of ties. ',
	r'With this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White & Rajkumar’s [28, 36] baseline generative model, but not with their averaged perceptron model. ',
	r'Therefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n-best parsing results, and per-label precision and recall for each type of dependency, together with the realizer’s normalized perceptron model score as a feature.'
	],
	['p14-1040',
	r'In this paper we present a grammar based approach for generating from knowledge bases (KB) which is linguistically principled and conceptually simple. ',
	r'To evaluate our approach, we use the benchmark provided by the KBGen challenge [4, 3], a challenge designed to evaluate generation from knowledge bases; where the input is a KB subset; and where the expected output is a complex sentence conveying the meaning represented by the input.'
	],
	['p14-1043',
	r'Supervised dependency parsing has made great progress during the past decade.',
	r'However, it is very difficult to further improve performance of supervised parsers.',
	r'In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest.',
	r'Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training ,co-training, and tri-training. ',
	r'However, these methods gain limited success in dependency parsing. ',
	r'Our experiments show that unlabeled data with identical outputs from different parsers tends to be short (18.25 words per sentence on average), and only has a small proportion of 40% (see Table 6). ',
	r'To solve above issues, this paper proposes a more general and effective framework for semi-supervised dependency parsing, referred to as ambiguity-aware ensemble training. ',
	r'To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser [], and a generative constituent parser []. ',
	r'Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3).',
	r'Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. ',
	r'Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ensemble training outperforms other entire-tree based methods such as self/co/tri-training. '
	],
	['p14-1044',
	r'On the one hand, these resources are heterogeneous in design, structure and content, but, on the other hand, they often provide complementary knowledge which we would like to see integrated. ',
	r'Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms.',
	r'Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions',
	r'When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. ',
	r'To do this, recent work has proposed graph construction by monosemous linking, where a concept is linked to all the concepts associated with the monosemous words in its definition ',
	r'However, this alignment method still involves tuning of parameters which are highly dependent on the characteristics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated.',
	r'In this paper we propose a unified approach to aligning arbitrary pairs of lexical resources which is independent of their specific structure.',
	r'Our approach also fares well when resources lack relational structure or pair-specific training data is absent, meaning that it is applicable to arbitrary pairs without adaptation. '
	],
	['p14-1045',
	r'The goal of the work presented in this paper is to improve distributional thesauri, and to help evaluate the content of such resources.',
	r'A distributional thesaurus is a lexical network that lists semantic neighbours, computed from a corpus and a similarity measure between lexical items, which generally captures the similarity of contexts in which the items occur.',
	r'There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations and this applies to distributional thesauri.',
	r'Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. ',
	r'They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard ',
	r'The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what call “non classical lexical semantic relations”.',
	r'At the same time, we want to filter associations that can be considered as accidental in a semantic perspective ',
	r'We hypothetize that evaluating and filtering semantic relations in texts where lexical items occur would help tasks that naturally make use of semantic similarity relations, but assessing this goes beyond the present work.'
	],
	['p14-1047',
	r'Building a dialogue policy can be a challenging task especially for complex applications.',
	r'For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies',
	r'Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user; (2) learn directly from a corpus; or (3) learn via live interaction with human users.',
	r'We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques',
	r'As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users.',
	r'So far research on using RL for dialogue policy learning has focused on single-agent RL techniques',
	r'Multi-agent RL is designed to work for non-stationary environments.',
	r'We apply multi-agent RL to a resource allocation negotiation scenario.',
	r'Our research contributions are as follows: (1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e., the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters).'
	],
	['p14-1048',
	r'Discourse parsing is the task of identifying the presence and the type of the discourse relations between discourse units.',
	r'Conventionally, there are two major sub-tasks related to text-level discourse parsing: (1) EDU segmentation: to segment the raw text into EDUs, and (2) tree-building: to build a discourse tree from EDUs, representing the discourse relations in the text.',
	r'The current state-of-the-art overall accuracy of the tree-building sub-task, evaluated on the RST Discourse Treebank (RST-DT, to be introduced in Section 8), is 55.73% by Joty et al. (2013). ',
	r'The main objective of this work is to develop a more efficient discourse parser, with similar or even better performance with respect to Joty et al.’s optimal parser, but able to produce parsing results in real time',
	r'Our contribution is three-fold',
	r'First, with a greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document.',
	r'Second, by using two linear-chain CRFs to label a sequence of discourse constituents, we can incorporate contextual information in a more natural way, compared to using traditional discriminative classifiers, such as SVMs.',
	r'Third, after a discourse (sub)tree is fully built from bottom up, we perform a novel post-editing process by considering information from the constituents on upper levels. '
	],
	['p14-1050',
	r'Automatic extraction of new words is indispensable to many tasks such as Chinese word segmentation, machine translation, named entity extraction, question answering, and sentiment analysis.',
	r'New word detection is one of the most critical issues in Chinese word segmentation',
	r'New word detection is also important for sentiment analysis such as opinionated phrase extraction and polarity classification.',
	r'Adding new words as feature in classification models will improve the performance of polarity classification, as demonstrated later in this paper.',
	r'This paper aims to detect new word for sentiment analysis.',
	r'Our central idea for new sentiment word detection is as follows: Starting from very few seed words (for example, just one seed word), we can extract lexical patterns that have strong statistical association with the seed words; the extracted lexical patterns can be further used in finding more new words, and the most probable new words can be added into the seed word set for the next iteration; and the process can be run iteratively until a stop condition is met. '
	],
	['p14-1052',
	r'Natural language generation (NLG) develops techniques to extend similar capabilities to automated systems.',
	r'In this paper, we study the restricted NLG problem: given a grammar, lexicon, world and a communicative goal, output a valid English sentence that satisfies this goal. ',
	r'The problem is restricted because in our work, we do not consider the issue of how to fragment a complex goal into multiple sentences (discourse planning).',
	r'Though restricted, this NLG problem is still difficult. ',
	r'Some NLG techniques use sampling strategies [8] where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal.',
	r'Other approaches view NLG as a planning problem ',
	r'This is an elegant formalization of NLG, however, restrictions on what current planning techniques can do limit its applicability. ',
	r'In our work, we also view NLG as a planning problem. ',
	r'However, we differ in that our underlying formalism for NLG is a suitably defined Markov decision process (MDP).'
	],
	['p14-1054',
	r'The relation recognition task is to find the relationships between two entities',
	r'The performance of relation extraction is still unsatisfactory with a F-score of 67.5% for English (23 subtypes). ',
	r'Chinese relation extraction also faces a weak performance having F-score about 66.6% in 18 subtypes',
	r'The difficulty of Chinese IE is that Chinese words are written next to each other without delimiter in between',
	r'Based on the characteristics of Chinese, in this paper, an Omni-word feature and a soft constraint method are proposed for Chinese relation extraction. ',
	r'We apply these approaches in a maximum entropy based system to extract relations from the ACE 2005 corpus.',
	r'Experimental results show that our method has made a significant improvement.'
	],
	['p14-1056',
	r'Automated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems',
	r'Hidden Markov models and linear-chain conditional random fields (CRFs) have previously been applied to citation extraction ',
	r'These models support efficient dynamic-programming inference, but only model local dependencies in the output label sequence.',
	r'However citations have strong global regularities not captured by these models',
	r'One framework for adding such global constraints into tractable models is constrained inference, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities.',
	r'When hard constraints can be encoded as linear equations on the output variables, and the underlying model’s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) ',
	r'Alternatively, one can employ dual decomposition',
	r'This paper introduces a novel method for imposing soft constraints via dual decomposition.',
	r'We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints.',
	r'Using our new method, we are able to incorporate not only all the soft global constraints of Chang et al. [4], but also far more complex data-driven constraints, while also providing stronger optimality certificates than their beam search technique.'
	],
	['p14-1057',
	r'Intuitively, to better solve this domain-specific retrieval problem, we need to understand the requirements specified in a query and identify the documents satisfying these requirements based on their semantic meanings.',
	r'Indeed, concept-based representation is one of the commonly used approaches that leverage knowledge bases to improve the retrieval performance',
	r'The basic idea is to represent both queries and documents as “bags of concepts”, where the concepts are identified based on the information from the knowledge bases.',
	r'In this paper, to address the limitation caused by the inaccurate concept mapping results, we propose to regularize the weighting strategies in the concept-based representation methods. ',
	r'We then propose two concept-based weighting regularization methods so that the regularized retrieval functions would satisfy the retrieval constraints and achieve better retrieval performance.',
	r'Experimental results over two TREC collections show that both proposed concept-based weighting regularization methods can improve the retrieval performance, and their performance is comparable with the best systems of the TREC Medical Records tracks'
	],
	['p14-1058',
	r'Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion, part-of-speech (POS) tagging and chunking, ontology learning, computing semantic textual similarity, and lexical inference.',
	r'However, the distribution of a word often varies from one domain to another.',
	r'In this paper, we use the term domain to refer to a collection of documents about a particular topic, for example reviews of a particular kind of product.',
	r'The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks.',
	r'The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks.',
	r'We propose a two-step unsupervised approach to predict the distribution of a word across domains. ',
	r'First, we create two lower dimensional latent feature spaces separately for the source and the target domains using Singular Value Decomposition (SVD). ',
	r'Second, we learn a mapping from the source domain latent feature space to the target domain latent feature space using Partial Least Square Regression (PLSR)',
	r'Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain.',
	r'Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. '
	],
	['p14-1060',
	r'This perspective of acquired meaning aligns with the ‘meaning is usage’ adage, consonant with Wittgenstein’s view of semantics. ',
	r'At the same time, the ability to adaptively communicate elaborate meanings can only be conciled through Frege’s principle of compositionality, i.e., meanings of larger linguistic constructs can be derived from the meanings of individual components, modulated by their syntactic interrelations.',
	r'It can be argued that to be sustainable, inductive aspects of meaning must be recurrent enough to be learnable by new users.',
	r'Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics.',
	r'However, while conventional DSMs consider collocation strengths (through counts and PMI scores) of word neighbourhoods, they disregard much of the regularity in human language.',
	r'We propose to identify such semantically cohesive motifs in terms of features inspired from frequency-characteristics, linguistic idiosyncrasies, and shallow syntactic analysis; and explore both supervised and semi-supervised models to optimally segment a sentence into such motifs.',
	r'Through exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design.'
	],
	['p14-1062',
	r'The aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation.',
	r'The core of a sentence model involves a feature function that defines the process by which the features of the sentence are extracted from the features of the words or n-grams.',
	r'The aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation.',
	r'Composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases.',
	r'Another approach represents the meaning of sentences by way of automatically extracted logical forms ',
	r'A central class of models are those based on neural networks.',
	r'These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations ',
	r'We define a convolutional neural network architecture and apply it to the semantic modelling of sentences.',
	r'The convolutional layers apply one-dimensional filters across each row of features in the sentence matrix. ',
	r'Multiple layers of convolutional and dynamic pooling operations induce a structured feature graph over the input sentence.',
	r'We experiment with the network in four settings',
	r'The first two experiments involve predicting the sentiment of movie reviews',
	r'The third experiment involves the categorisation of questions in six question types in the TREC dataset ',
	r'The fourth experiment involves predicting the sentiment of Twitter posts using distant supervision '
	],
	['p14-1063',
	r'This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably.',
	r'In this paper, we shift the model from vector-space to tensor-space.',
	r'A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. ',
	r'With this representation, we no longer need to estimate individual feature weights directly but only a small number of “bases” instead.',
	r'This property makes the the tensor model very effective when training a large number of feature weights in a low-resource environment. ',
	r'On the other hand, tensor models have many more degrees of “design freedom” than vector space models'
	],
	['p14-1064',
	r'Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities.',
	r'However, the limiting factor in the success of these techniques is parallel data availability.',
	r'While parallel data is generally scarce, monolingual resources exist in abundance and are being created at accelerating rates.',
	r'Our work introduces a new take on the problem using graph-based semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources.',
	r'Unlike previous work, we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text.',
	r'We then limit the set of translation options for each unlabeled source phrase, and using a structured graph propagation algorithm, where translation information is propagated from labeled to unlabeled phrases proportional to both source and target phrase similarities, we estimate probability distributions over translations for the unlabeled source phrases.'
	],
	['p14-1065',
	r'Recently, there have been two promising research directions for improving SMT and its evaluation: (a)  by using more structured linguistic information, such as syntax, hierarchical structures, and semantic roles, and (b)  by going beyond the sentence-level, e.g., translating at the document level.',
	r'Thus, in a coherent text, discourse units (sentences or clauses) are logically connected: the meaning of a unit relates to that of the previous and the following units.',
	r'Discourse analysis seeks to uncover this coherence structure underneath the text',
	r'Modeling discourse brings together the above research directions (a) and (b), which makes it an attractive goal for MT.',
	r'The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation.',
	r'We believe that the semantic and pragmatic information captured in the form of DTs (i)  can help develop discourse-aware SMT systems that produce coherent translations, and (ii)  can yield better MT evaluation metrics.',
	r'In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. '
	],
	['p14-1067',
	r'The possibility to speed up the translation process and reduce its costs by post-editing good-quality MT output raises interesting research challenges. ',
	r'In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations ',
	r'Despite the substantial progress done so far in the field and in successful evaluation campaigns, focusing on concrete market needs makes possible to further define the scope of research on QE.',
	r'To cope with these issues and deal with the erratic conditions of real-world translation workflows, we propose an adaptive approach to QE that is sensitive and robust to differences between training and test data.',
	r'Along this direction, our main contribution is a framework in which QE models can be trained and can continuously evolve over time accounting for knowledge acquired from post editors’ work.',
	r'Our approach is based on the online learning paradigm and exploits a key difference between such framework and the batch learning methods currently used.',
	r'To develop our approach, different online algorithms have been embedded in the backbone of a QE system. ',
	r'Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach.',
	r'Our results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework.'
	],
	['p14-1068',
	r'In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis: words that appear in similar linguistic contexts are likely to have related meanings',
	r'Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities.',
	r'These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images).',
	r'In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by mapping words and images into a common embedding space.',
	r'Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. ',
	r'We follow  in arguing that an attribute-centric representation is expedient for several reasons.',
	r'Our model learns multimodal representations from attributes which are automatically inferred from text and images.',
	r'We evaluate the embeddings it produces on two tasks, namely word similarity and categorization.'
	],
	['p14-1070',
	r'A real-life parsing system should comprise the recognition of multi-word expressions ',
	r'Multiword expressions can be roughly defined as continuous or discontinuous sets of tokens, which either do not exhibit full freedom in lexical selection or whose meaning is not fully compositional',
	r'While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing, the French dataset of the SPMRL 2013 Shared Task only recently provided the opportunity to evaluate this scenario within the framework of dependency syntax',
	r'In this paper, we investigate various strategies for predicting from a tokenized sentence both MWEs and syntactic dependencies, using the French dataset of the SPMRL 13 Shared Task. '
	],
	['p14-1073',
	r'Document context is needed to determine whether they may be talking about two different people.',
	r'In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems.',
	r'Our model is an evolutionary generative process based on the name variation model of , which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). ',
	r'Our model learns without supervision that these all refer to the the same entity.',
	r'Our primary contributions are improvements on  for the entity clustering task.',
	r'We evaluate our approach by comparing to several baselines on datasets from three different genres: Twitter, newswire, and blogs'
	],
	['p14-1077',
	r'In the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored.',
	r'All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE',
	r'However, properly capturing and utilizing such typing clues are not trivial. ',
	r'On the other hand, most previous relation extractors process each entity pair (we will use entity pair and entity tuple exchangeably in the rest of the paper) locally and individually, i.e., the extractor makes decisions solely based on the sentences containing the current entity pair and ignores other related pairs, therefore has difficulties to capture possible disagreements among different entity pairs.',
	r'In this paper, we will address how to derive and exploit two categories of these clues: the expected types and the cardinality requirements of a relation’s arguments, in the scenario of relation extraction. ',
	r'We propose to perform joint inference upon multiple local predictions by leveraging implicit clues that are encoded with relation specific requirements and can be learnt from existing knowledge bases. ',
	r'The experimental results show that our framework performs better than the state-of-the-art approaches when such clues are applicable to the datasets',
	r'We also show that the automatically learnt clues perform comparably to those refined manually.'
	],
	['p14-1079',
	r'Relation Extraction (RE) is the process of generating structured relation knowledge from unstructured natural language texts. ',
	r'To address the lacking training data issue, we consider the distant or weak supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper.',
	r'The intuition of the paradigm is that one can take advantage of several knowledge bases, such as WordNet,Freebase and YAGO,to automatically label free texts, like Wikipedia and New York Times corpora,based on some heuristic alignment assumptions. ',
	r'In essence, distantly supervised relation extraction is an incomplete multi-label classification task with sparse and noisy features.',
	r'In this paper, we formulate the relation-extraction task from a novel perspective of using matrix completion with low rank criterion',
	r'We contribute two optimization models, DRMC and DRMC-1, aiming at exploiting the sparsity to recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously. ',
	r'Experiments on two widely used datasets demonstrate that our noise-tolerant approaches outperform the baseline and the state-of-the-art methods. '
	],
	['p14-1082',
	r'Whereas machine translation generally concerns the translation of whole sentences or texts from one language to the other, this study focusses on the translation of native language (henceforth L1) words and phrases, i.e. smaller fragments, in a foreign language (L2) context.',
	r'Despite the major efforts and improvements, automatic translation does not yet rival human-level quality.',
	r'The cross-lingual context in our research question may at first seem artificial, but its design explicitly aims at applications related to computer-aided language learning [] and computer-aided translation ',
	r'The proposed application allows code switching and produces context-sensitive suggestions as writing progresses. ',
	r'In this research we test the feasibility of the foundation of this idea.',
	r'The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models.'
	],
	['p14-1083',
	r'In this paper, we propose a novel approach for learning and evaluation in statistical machine translation (SMT) that borrows ideas from response-based learning for grounded semantic parsing.',
	r'We suggest that in a similar way the preservation of meaning in machine translation should be defined in the context of an interaction in an extrinsic task.',
	r'We propose a framework of response-based learning that allows to extract supervision signals for structured learning from the response of an extrinsic task to a translation input.',
	r'Here, learning proceeds by “trying out” translation hypotheses, receiving a response from interacting in the task, and converting this response into a supervision signal for updating model parameters. ',
	r'In this paper, we present a proof-of-concept experiment that uses feedback from a simulated world environment.',
	r'Our experimental results show an improvement of about 6 points in F1-score for response-based learning over standard structured learning from reference translations.',
	r'We show in an error analysis that this improvement can be attributed to using structural and lexical variants of reference translations as positive examples in response-based learning.'
	],
	['p14-1085',
	r'The explosion in the number of documents on the Web necessitates automated approaches that organize and summarize large document collections on a complex topic.',
	r'MDS systems do not scale to data sets ten times larger and proportionately longer summaries: they either cannot run on large input or produce a disorganized summary that is difficult to understand.',
	r'We present a novel MDS paradigm, hierarchical summarization, which operates on large document collections, creating summaries that organize the information coherently.',
	r'It mimics how someone with a general interest in a complex topic would learn about it from an expert – first, the expert would provide an overview, and then more specific information about various aspects.',
	r'In this paper, we describe Summa, the first hierarchical summarization system for multi-document summarization.',
	r'Summa hierarchically clusters the sentences by time, and then summarizes the clusters using an objective function that optimizes salience and coherence',
	r'Summa hierarchically clusters the sentences by time, and then summarizes the clusters using an objective function that optimizes salience and coherence'
	],
	['pp14-1087',
	r'There has been a good amount of research invested into improving the temporal interpretation of text.',
	r'In this paper, we present our work in incorporating the use of such temporal information into multi-document summarization.',
	r'In this paper, we present our work in incorporating the use of such temporal information into multi-document summarization.',
	r'Our work is significant as it addresses an important gap in the exploitation of temporal information. ',
	r'In this work we construct timelines (as a representation of temporal information) automatically and incorporate them into a state-of-the-art multi-document summarization system.',
	r'This is achieved with 1) three novel features derived from timelines to help measure the saliency of sentences, as well as 2) TimeMMR, a modification to the traditional Maximal Marginal Relevance ',
	r'Compared to a competitive baseline, significant improvements of up to 4.1% are obtained',
	r'This work additionally proposes the use of the lengths of timelines as a metric to gauge the usefulness of timelines.'
	],
	['p14-1088',
	r'It is a truth universally acknowledged that an annotation task in good standing be in possession of a measure of inter-annotator agreement (IAA).',
	r'However, no such measure is in widespread use for the task of syntactic annotation.',
	r'In this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff’s α and tree edit distance.'
	],
	['p14-1090',
	r'With the recent growth in KBs such as DBPedia [1], Freebase [4] and Yago2 [18], it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now, based on Google’s Knowledge Graph, and Facebook Graph Search, based on social network connections.',
	r'The AI community has tended to approach this problem with a focus on first understanding the intent of the question, via shallow or deep forms of semantic parsing (c.f. §3 for a discussion).',
	r'Performance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries',
	r'The Information Extraction (IE) community approaches QA differently: first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates, and only then attempting to perform deeper analysis.',
	r'While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that “traditional” IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of  34% F1 as compared to Berant et al'
	],
	['p14-1091',
	r'Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs).',
	r'Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly.',
	r'Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure. ',
	r'The contributions of this work are two-fold:',
	r'We propose a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework',
	r'We propose a robust method to transform single-relation questions into formal triple queries as their MRs, which trades off between transformation accuracy and recall using question patterns and relation expressions respectively.'
	],
	['p14-1092',
	r'However, most of this effort has focused on factoid questions rather than more complex non-factoid (NF) questions, such as manner, reason, or causation questions. ',
	r'Moreover, the vast majority of QA models explore only local linguistic structures, such as syntactic dependencies or semantic role frames, which are generally restricted to individual sentences.',
	r'This is problematic for NF QA, where questions are answered not by atomic facts, but by larger cross-sentence conceptual structures that convey the desired answers.',
	r'Thus, to answer NF questions, one needs a model of what these answer structures look like.',
	r'We propose a novel answer reranking (AR) model that combines lexical semantics (LS) with discourse information, driven by two representations of discourse: a shallow representation centered around discourse markers and surface text information, and a deep one based on the Rhetorical Structure Theory (RST) discourse framework [7]. ',
	r'We demonstrate that modeling discourse is greatly beneficial for NF AR for two types of NF questions, manner (“how”) and reason (“why”), across two large datasets from different genres and domains – one from the community question-answering (CQA) site of Yahoo! Answers and one from a biology textbook. Our results show statistically significant improvements of up to 24% on top of state-of-the-art LS models.',
	r'We demonstrate that both shallow and deep discourse representations are useful, and, in general, their combination performs best',
	r'We show that discourse-based QA models using inter-sentence features considerably outperform single-sentence models when answers span multiple sentences.',
	r'We demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA.'
	],
	['p14-1093',
	r'Our ultimate goal is to develop a system that supports scenario planning through generating possible future events using big data, which would contain what Donald Rumsfeld called “unknown unknowns”',
	r'To this end, we propose a supervised method of extracting such event causality as conduct slash-and-burn agriculture→exacerbate desertification and use its output to generate future scenarios (scenarios), which are chains of causality that have been or might be observed in this world like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse. ',
	r'Our method extracts event causality based on three assumptions that are embodied as features of our classifier. ',
	r'First, we assume that two nouns (e.g. slash-and-burn agriculture and desertification) that take some specific binary semantic relations (e.g. A causes B) tend to constitute event causality if combined with two predicates (e.g. conduct and exacerbate). ',
	r'Our second assumption is that there are grammatical contexts in which event causality is more likely to appear.',
	r'The last assumption embodied in our association features is that each word of the cause phrase must have a strong association (i.e., PMI, for example) with that of the effect phrase as slash-and-burn agriculture and desertification in the above example, as in Do et al. ',
	r'We require that event causality be self-contained, i.e., intelligible as causality without the sentences from which it was extracted. ',
	r'Our scenario generation method generates scenarios by chaining extracted event causality; generating A→B→C from A→B and B→C. ',
	r'To increase the number of acceptable scenarios, our method identifies compatibility w.r.t causality between two phrases by a recently proposed semantic polarity, excitation [12], which properly relaxes the chaining condition (Section 3.1 describes it). '
	],
	['p14-1096',
	r'Words take different senses in different contexts while appearing with other words.',
	r'Automatic discovery and disambiguation of word senses from a given text is an important and challenging problem which has been extensively studied in the literature',
	r'However, another equally important aspect that has not been so far well investigated corresponds to one or more changes that a word might undergo in its sense.',
	r'Note that this phenomena of change in word senses has existed ever since the beginning of human communication ; however, with the advent of modern technology and the availability of huge volumes of time-varying data it now has become possible to automatically track such changes and, thereby, help the lexicographers in word sense discovery, and design engineers in enhancing various NLP/IR applications (e.g., disambiguation, semantic search etc.) that are naturally sensitive to change in word senses.hanges in large texts available over multiple timescales. ',
	r'The above motivation forms the basis of the central objective set in this paper, which is to devise a completely unsupervised approach to track noun sense changes in large texts available over multiple timescales. ',
	r'Toward this objective we make the following contributions: (a) devise a time-varying graph clustering based sense induction algorithm, (b) use the time-varying sense clusters to develop a split-join based approach for identifying new senses of a word, and (c) evaluate the performance of the algorithms on various datasets using different suitable approaches along with a detailed error analysis.',
	r'Remarkably, comparison with the English WordNet indicates that in 44% cases, as identified by our algorithm, there has been a birth of a completely novel sense, in 46% cases a new sense has split off from an older sense and in 43% cases two or more older senses have merged in to form a new sense.'
	],
	['p14-1097',
	r'Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP Verb classes are one such lexical resource. ',
	r'There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages.',
	r'In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. ',
	r'Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. '
	],
	['p14-1100',
	r'Solutions to the problem of grammar induction have been long sought after since the early days of computational linguistics and are interesting both from cognitive and engineering perspectives. ',
	r'Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars, and the constituent context model',
	r'Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood or a variant of i',
	r'These approaches, while empirically promising, generally lack theoretical justification.',
	r'These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results.',
	r'In this paper, we suggest a different approach, to provide a first step to bridging this theory-experiment gap. ',
	r'More specifically, we approach unsupervised constituent parsing from the perspective of structure learning as opposed to parameter learning. ',
	r'In our framework, parsing reduces to finding the best latent structure for a given sentence.',
	r'Intuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of “spectral” methods that can lead to provably correct solutions. ',
	r'Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). ',
	r'Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. ',
	r'Empirically we evaluate our method on data in English, German and Chinese. ',
	r'Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization.',
	r'In addition, we also analyze CCM’s sensitivity to initialization, and compare our results to Seginer’s algorithm'
	],
	['p14-1101',
	r'However, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories, most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information',
	r'These findings indicate that young infants are sensitive to co-occurrences between linguistic stimuli and at least some aspects of the world.',
	r'In this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear (as in the lexical-phonetic learning model of 11) and also to the situations in which word-forms are used.',
	r'However, in our simulations we approximate the environmental information by running a topic model over a corpus of child-directed speech to infer a topic distribution for each situation.',
	r'These topic distributions are then used as input to our model to represent situational contexts.',
	r'The situational information in our model is similar to that assumed by theories of cross-situational word learning, but our model does not require learners to map individual words to their referents',
	r'These results demonstrate that relying on situational co-occurrence can improve phonetic learning, even if learners do not yet know the meanings of individual words.'
	],
	['p14-1102',
	r'The phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g. [the apple]i that the boy ate ti or [what]i did the boy eat ti), has long been an object of study for syntacticians [] due to its apparent processing complexity.',
	r'This work describes a cognitive model of the developmental timecourse of filler-gap comprehension with the goal of setting a lower bound on the modeling assumptions necessary for an ideal learner to display filler-gap comprehension.',
	r'This approach to filler-gap comprehension is supported by findings that show people do not actually link fillers to gap positions but instead link the filler to a verb with missing arguments'
	],
	['p14-1104',
	r'This is a challenge faced by many real world applications – given a large, quickly and cheaply created, low quality annotated dataset, how can one improve its quality and learn an accurate classifier from it?',
	r'An active learner uses a small set of labeled data to iteratively select the most informative instances from a large pool of unlabeled data for human annotators to label',
	r'In this work, we borrow the idea of active learning to interactively and iteratively correct labeling errors.',
	r'Specifically, we propose a novel non-linear distribution spreading algorithm, which first uses Delta IDF technique [11] to weight features, and then leverages the distribution of Delta IDF scores of a feature across different classes to efficiently recognize discriminative features for the classification task in the presence of mislabeled data.',
	r'Extensive experiments show that, the proposed techniques are as effective as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning.'
	],
	['p14-1105',
	r'Manually identifying ideological bias in political text, especially in the age of big data, is an impractical and expensive process.',
	r'In this paper, we examine the problem of detecting ideological bias on the sentence level.',
	r'Ideological bias is difficult to detect, even for humans—the task relies not only on political knowledge but also on the annotator’s ability to pick up on subtle elements of language use.',
	r'Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. ',
	r'In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects',
	r'Building from those insights, we introduce a recursive neural network (rnn) to detect ideological bias on the sentence level.',
	r'This model requires richer data than currently available, so we develop a new political ideology dataset annotated at the phrase level. ',
	r'With this new dataset we show that rnns not only label sentences well but also improve further when given additional phrase-level annotations. rnns are quantitatively more effective than existing methods that use syntactic and semantic features separately, and we also illustrate how our model correctly identifies ideological bias in complex syntactic constructions.'
	],
	['p14-1106',
	r'However, reordering, especially without any help of external knowledge, remains a great challenge because an accurate reordering is usually beyond these word level or translation phrase level reordering models’ ability.',
	r'In this paper our goal is to take advantage of syntactic and semantic parsing to improve translation quality. ',
	r'To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-of-the-art HPB system ',
	r'Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information.',
	r'To this end, we employ the same reordering framework as syntactic constituent reordering and focus on semantic roles in a PAS.',
	r'We then analyze the differences between the syntactic and semantic features.',
	],
	['p14-1107',
	r'SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data.',
	r'There are various options for creating training data for new language pairs. ',
	r'Until relatively recently, little consideration has been given to creating parallel data from scratch.',
	r'Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. ',
	r'Rather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk (MTurk) platform ',
	r'To sort good translations from bad, researchers often solicit multiple, redundant translations and then build models to try to predict which translations are the best, or which translators tend to produce the highest quality translations.'
	],
	['p14-1108',
	r'Language Models are a probabilistic approach for predicting the occurrence of a sequence of words.',
	r'The challenge in the construction of language models is to provide reliable estimators for the conditional probabilities.',
	r'Smoothing is a standard technique to overcome this data sparsity problem. ',
	r'Chen and Goodman [] introduced modified Kneser-Ney Smoothing, which up to now has been considered the state-of-the-art method for language modelling over the last 15 years.',
	r'Because of Zipfian word distributions, most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly. ',
	r'Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning.',
	r'Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways.'
	],
	['p14-1109',
	r'Today, even though earnings calls transcripts are abundantly available, their distinctive communicative practices, and correlations with the financial risks, in particular, future stock performances, are not well studied in the past.',
	r'Earnings calls are conference calls where a listed company discusses the financial performance',
	r'Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk',
	r'To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies.',
	r'To evaluate the performance of our approach, we compare with a standard squared loss linear regression baseline, as well as strong baselines such as linear and non-linear support vector machines (SVMs) that are widely used in text regression tasks. ',
	r'By varying different experimental settings on three datasets concerning different periods of the Great Recession from 2006-2013, we empirically show that our approach significantly outperforms the baselines by a wide margin.'
	],
	['p14-1113',
	r'Semantic hierarchies are natural ways to organize knowledge. ',
	r'However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming.',
	r'Therefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies.',
	r'A major challenge for this task is the automatic discovery of hypernym-hyponym relations. ',
	r'This paper proposes a novel approach for semantic hierarchy construction based on word embeddings.',
	r'However, we further observe that hypernym–hyponym relations are more complicated than a single offset can represent.',
	r'To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms',
	r'Furthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym–hyponym relations ',
	r'Subsequently, we identify whether an unknown word pair is a hypernym–hyponym relation using the projections ',
	r'The experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the previous state-of-the-art methods.'
	],
	['p14-1115',
	r'However, often a good summary cannot be generic and should be a brief and well-organized paragraph that answer a user’s information need.',
	r'The Document Understanding Conference (DUC) has launched query-focused multidocument summarization as its main task since 2004, by focusing on complex queries with very specific answers.',
	r'However, especially when dealing with conversational data that tend to be less structured and less topically focused, a user is often initially only exploring the source documents, with less specific information needs. ',
	r'To address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries. ',
	r'To date, most systems in the area of summarization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited.',
	r'Moreover, most of the proposed systems for conversation summarization are extractive.',
	r'To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization.',
	r'We evaluate our system over GNUe Traffic archive Internet Relay Chat (IRC) logs, AMI meetings corpus [4] and BC3 emails dataset [26]. ',
	r'Automatic evaluation on the chat dataset and manual evaluation over the meetings and emails show that our system uniformly and statistically significantly outperforms baseline systems, as well as a state-of-the-art query-based extractive summarization system.'
	],
	['p14-1116',
	r'Summarisation of time-series data refers to the task of automatically generating text from variables whose values change over time.',
	r'In this work, we concentrate on content selection which is the task of choosing what to say, i.e. what information is to be included in a report',
	r'We frame content selection as a simple classification task: given a set of time-series data, decide for each template whether it should be included in a summary or not. ',
	r'However, simple classification assumes that the templates are independent of each other, thus the decision for each template is taken in isolation from the others, which is not appropriate for our domain. ',
	r'In order to capture the dependencies in the context, multiple simple classifiers can make the decisions for each template iteratively. ',
	r'Here, we propose an alternative method that tackles the challenge of interdependent data by using multi-label (ML) classification, which is efficient in taking data dependencies into account and generating a set of labels (in our case templates) simultaneously',
	r'Our contributions to the field are as follows: we present a novel and efficient method for tackling the challenge of content selection using a ML classification approach; we applied this method to the domain of feedback summarisation; we present a comparison with an optimisation technique (Reinforcement Learning), and we discuss the similarities and differences between the two methods.'
	],
	['p14-1117',
	r'Sentence compression is a text-to-text generation task in which an input sentence must be transformed into a shorter output sentence which accurately reflects the meaning in the input and also remains grammatically well-formed. ',
	r'A number of diverse approaches have been proposed for deletion-based sentence compression, including techniques that assemble the output text under an n-gram factorization over the input text [36, 7] or an arc factorization over input dependency parses [16, 17, 15].',
	r'Joint methods have also been proposed that invoke integer linear programming (ILP) formulations to simultaneously consider multiple structural inference problems—both over n-grams and input dependencies [34] or n-grams and all possible dependencies',
	r'In this work, we develop approximate inference strategies to the joint approach of Thadani and McKeown (2013) which trade the optimality guarantees of exact ILP for faster inference by separately solving the n-gram and dependency subproblems and using Lagrange multipliers to enforce consistency between their solutions. ',
	r'However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non-projective subtrees in a general directed graph.',
	r'We therefore consider methods to recover approximate solutions for the subproblem of finding the maximum weighted subtree in a graph, common among which is the use of a linear programming relaxation.',
	r'Our proposed approximation strategies are evaluated using automated metrics in order to address the question: under what conditions should a real-world sentence compression system implementation consider exact inference with an ILP or approximate inference? '
	],
	['p14-1118',
	r'YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with a social graph, and discussions between people sharing an interest.',
	r'Hence, doing sentiment research in such an environment is highly relevant for the community.',
	r'While previous state-of-the-art models for opinion classification have been successfully applied to traditional corpora [15], YouTube comments pose additional challenges: (i) polarity words can refer to either video or product while expressing contrasting sentiments; (ii) many comments are unrelated or contain spam; and (iii) learning supervised models requires training data for each different YouTube domain, e.g., tablets, automobiles, etc. ',
	r'In this paper, we carry out a systematic study on OM targeting YouTube comments; its contribution is three-fold: firstly, to solve the problems outlined above, we define a classification schema, which separates spam and not related comments from the informative ones, which are, in turn, further categorized into video- or product-related comments (type classification).',
	r'The second contribution of the paper is the creation and annotation (by an expert coder) of a comment corpus containing 35k manually labeled comments for two product YouTube domains: tablets and automobiles',
	r'The third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g., iPad, Kindle, Toyota Camry.',
	r'Finally, our results show that our models are adaptable, especially when the structural information is used.'
	],
	['p14-1119',
	r'Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” ',
	r'Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization [62], text categorization [19], opinion mining [2], and document indexing [14].',
	r'However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks',
	r'Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. '
	],
	['p14-1121',
	r'According to Fung and Cheung (2004), who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation).',
	r'The bilingual lexicon extraction task from comparable corpora inherits this filiation.',
	r'In this paper we want to show that the assumption that comparable corpora should be balanced for bilingual lexicon extraction task is unfounded.',
	r'Within this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the unfounded assumption of the balance of the specialized comparable corpora. ',
	r'Consequently, the observations of word co-occurrences which is the basis of the standard approach are unreliable.',
	r'To make them more reliable, our second contribution is to contrast different regression models in order to boost the observations of word co-occurrences.'
	],
	['p14-1122',
	r'However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required',
	r'Recent approaches have attempted to build or extend these knowledge bases automatically.',
	r'In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. ',
	r'In contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with. ',
	r'The fun nature of the games provides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation.',
	r'Our work provides the following three contributions.',
	r'First, we demonstrate effective video game-based methods for both validating and extending semantic networks, using two games that operate on complementary sources of information: semantic relations and sense-image mappings. ',
	r'Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing.',
	r'Third, for both games, we show that games produce better quality annotations than crowdsourcing.'
	],
	['p14-1123',
	r'Prior studies in language acquisition and second language research have conclusively shown that proficiency in a second language is characterized by several factors, some of which are, fluency in language production, pronunciation accuracy, choice of vocabulary, grammatical sophistication and accuracy. ',
	r'The design of automated scoring systems for non-native speaker speaking proficiency is guided by these studies in the choice of pertinent objective measures of these key aspects of language proficiency.',
	r'The focus of this study is the design and performance analysis of a measure of the syntactic complexity of non-native English responses for use in automatic scoring systems.',
	r'This study is different from studies that focus on capturing grammatical errors in non-native speakers ',
	r'Instead of focusing on grammatical errors that are found to be highly representative of language proficiency, our interest is in capturing the range of forms that surface in language production and the degree of sophistication of such forms, collectively referred to as syntactic complexity in',
	r'Guided by studies in second language development, we design a measure of syntactic complexity that captures patterns indicative of proficient and non-proficient grammatical structures by a shallow-analysis of spoken language, as opposed to a deep syntactic analysis, and analyze the performance of the automatic scoring model with its inclusion.'
	],
	['p14-1124',
	r'Given a small vocabulary of interest (1000-2000 words or multi-word terms) the aim of the term detection task is to enumerate occurrences of the keywords within a target corpus.',
	r'Spoken term detection converts the raw acoustics into time-marked keyword occurrences, which may subsequently be fed (e.g. as a bag-of-terms) to standard NLP algorithms.',
	r'Although spoken term detection does not require the use of word-based automatic speech recognition (ASR), it is closely related. ',
	r'In order to improve detection performance, and restricting ourselves to an existing ASR system or systems at our disposal, we focus on leveraging broad document context around detection hypotheses.',
	r'We will show that by focusing on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program.',
	r'We evaluate term detection and word repetition-based re-scoring on the IARPA BABEL training and development corpora for five languages Cantonese, Pashto, Turkish, Tagalog and Vietnamese',
	r'In order to arrive at our eventual solution, we take the BABEL Tagalog corpus and analyze word co-occurrence and repetition statistics in detail.',
	r'We then analyze the tendency towards within-document repetition.',
	r'We then demonstrate that the method generalizes well, by applying it to the 2006 English data and the remaining four 2013 BABEL languages. '
	],
	['p14-1125',
	r'Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures ',
	r'Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters.',
	r'Character-level dependency parsing is interesting in at least two aspects.',
	r'First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation.',
	r'Second, word internal structures can also be useful for syntactic parsing. ',
	r'In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework',
	r'We conduct experiments on three data sets, including CTB 5.0, CTB 6.0 and CTB 7.0. ',
	r'Experimental results show that the character-level dependency parsing models outperform the word-based methods on all the data sets. '
	],
	['p14-1126',
	r'In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation [43], relation extraction [37] and machine translation [21, 51].',
	r'In this paper, we consider a practically motivated scenario, in which we want to build statistical parsers for resource-poor target languages, using existing resources from a resource-rich source language (like English).',
	r'We assume that there are absolutely no labeled training data for the target language, but we have access to parallel data with a resource-rich language and a sufficient amount of labeled training data to build an accurate parser for the resource-rich language',
	r'This scenario is applicable to a large set of languages and many research studies [19] have been made on it',
	r'In this work, we propose a learning framework for transferring dependency grammars from a resource-rich language to resource-poor languages via parallel text. ',
	r'We train probabilistic parsing models for resource-poor languages by maximizing a combination of likelihood on parallel data and confidence on unlabeled data. ',
	r'Our work is based on the learning framework used in Smith and Eisner [44], which is originally designed for parser bootstrapping. We extend this learning framework so that it can be used to transfer cross-lingual knowledge between different languages.',
	r'Our approach achieves significant improvement over previous state-of-the-art unsupervised and projected parsing systems across all the ten languages, and considerably bridges the gap to fully supervised dependency parsing performance.'
	],
	['p14-1128',
	r'The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the hand-annotated treebank data, e.g., Chinese treebank (CTB) ',
	r'But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task.',
	r'In recent years, a number of works attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data.',
	r'The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment [15]. ',
	r'However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation.',
	r'This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmentation nature.'
	],
	['p14-1129',
	r'In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature.',
	r'Additionally, we present several variations of this model which provide significant additive BLEU gains.',
	r'We also present a novel technique for training the neural network to be self-normalized, which avoids the costly step of posteriorizing over the entire vocabulary in decoding.',
	r'Although our model is quite simple, we obtain strong empirical results'
	],
	['p14-1130',
	r'Traditionally, parsing research has focused on modeling the direct connection between the features and the predicted syntactic relations such as head-modifier (arc) relations in dependency parsing.',
	r'The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data.',
	r'A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features ',
	r'We depart from this view and leverage high-dimensional feature vectors by mapping them into low dimensional representations. ',
	r'We implement the low-rank factorization model in the context of first- and third-order dependency parsing',
	r'The model was evaluated on 14 languages, using dependency data from CoNLL 2008 and CoNLL 2006.',
	r'The low-rank parser achieves average performance of 89.08% across 14 languages, compared to 88.73% for the Turbo parser, and 87.19% for MST. ',
	r'Finally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines.'
	],
	['p14-1131',
	r'This paper introduces CoSimRank, a new graph-theoretic algorithm for computing node similarity that combines features of SimRank and PageRank. ',
	r'Our key observation is that to compute the similarity of two nodes, we need not consider all other nodes in the graph as SimRank does; instead, CoSimRank starts random walks from the two nodes and computes their similarity at each time step.',
	r'Thus, CoSimRank has the added advantage of being a flexible tool for different types of applications.',
	r'The extension of CoSimRank to similarity across graphs is important for the application of bilingual lexicon extraction: given a set of correspondences between nodes in two graphs A and B (corresponding to two different languages), a pair of nodes (a∈A,b∈B) is a good candidate for a translation pair if their node similarity is high. ',
	r'In an experimental evaluation, we show that CoSimRank is more efficient and more accurate than both SimRank and PageRank-based algorithms.'
    ],
	['p14-1132',
	r'Computational models of meaning that rely on corpus-extracted context vectors, such as LSA [31], HAL [36], Topic Models [20] and more recent neural-network approaches [11, 38] have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness [53]. ',
	r'However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external world.',
	r'But lack of reference is not only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions [3, 10], that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges.',
	r'In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the corresponding words.',
	r'This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space.',
	r'We first test the effectiveness of our cross-modal semantic space on the so-called zero-shot learning task [40], which has recently been explored in the machine learning community [18, 49]. ',
	r'Our approach is competitive with respect to the recently proposed alternatives, while being overall simpler',
	r'We show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made, even when the linguistic context vector representing the word has been created from as little as 1 sentence containing it'
	],
	['p14-1133',
	r'We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB)',
	r'To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. ',
	r'However, this leaves untapped a vast amount of text not related to the KB.',
	r'In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB',
	r'We use two complementary paraphrase models: an association model based on aligned phrase pairs extracted from a monolingual parallel corpus, and a vector space model, which represents each utterance as a vector and learns a similarity score between them.',
	r'Our work relates to recent lines of research in semantic parsing and question answering.',
	r'In some sense, we approach the problem from the opposite end, using an intermediate utterance, which allows us to employ paraphrasing methods',
	r'We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase).',
	r'We apply our semantic parser on two datasets: WebQuestions [1], which contains 5,810 question-answer pairs with common questions asked by web users; and Free917 [2], which has 917 questions manually authored by annotators.',
	r'On WebQuestions, we obtain a relative improvement of 12% in accuracy over the state-of-the-art, and on Free917 we match the current best performing system. '
	],
    ['P14-1136',
     r'We present a new technique for semantic frame identification that leverages distributed word representations. ',
     r'According to the theory of frame semantics [12], a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the event.',
     r'Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles [8, 7],.11',
     r'Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles [8, 7].11',
     r'Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing.',
     r'First, we show that for frame identification on the FrameNet corpus [2, 11], we outperform the prior state of the art [7].',
     r'Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date.',
     r'Second, we present results on PropBank-style semantic role labeling [22, 19, 21], that approach strong baselines, and are on par with prior state of the art [23].'
    ],
    ['P14-1138',
     r'Automatic word alignment is an important task for statistical machine translation.',
     r'The most classical approaches are the probabilistic IBM models 1-5 [4] and the HMM model [39].',
     r'Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks [24, 25, 1, 16, 34].',
     r'An RNN has a hidden layer with recurrent connections that propagates its own previous signals.s',
     r'We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model.',
     r'Unfortunately, it is usually difficult to prepare word-by-word aligned bilingual data.',
     r'To solve this problem, we apply noise-contrastive estimation (NCE) [15, 26] for unsupervised training of our RNN-based model without gold standard alignments or pseudo-oracle alignments.',
     r'NCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences.',
     r'The results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks.',
     r'For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4.'
    ],

    ['P14-1140',
     r'Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first.',
     r'Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time [12].',
     r'Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing [16], and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics [15].',
     r'DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. ',
     r'R2NN is a combination of recursive neural network and recurrent neural network.',
     r'In R2NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure can be built, as recursive neural networks.',
     r'We propose a three-step semi-supervised training approach to optimizing the parameters of R2NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy. ',
     r'So as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network.',
     r'We conduct experiments on a Chinese-to-English translation task to test our proposed methods, and we get about 1.5 BLEU points improvement, compared with a state-of-the-art baseline system.'
    ],

    ['P14-1142',
     r'An IME is an essential software interface that maps Chinese characters into English letter combinations.',
     r'Nowadays most of Chinese IMEs are pinyin based',
     r'Pinyin is originally designed as the phonetic symbol of a Chinese character (based on the standard modern Chinese, mandarin) , using Latin letters as its syllable notation.',
     r'But there are only less than 500 pinyin syllables in standard modern Chinese, compared with over 6,000 commonly used Chinese characters, which leads to serious ambiguities for pinyin-to-character mapping.',
     r'Thus there are two separated sub-tasks for Chinese spell checking: 1. typo checking for user typed pinyin sequences which should be a built-in module in IME, and 2. spell checking for Chinese texts in its narrow sense, which is typically a module of word processing applications [36]. ',
     r'The user may fail to input the completely right pinyin simply because he/she is a dialect speaker and does not know the exact pronunciation for the expected character.',
     r'However, existing practical IMEs only provide small patches to deal with typos such as Fuzzy Pinyin [33] and other language specific errors [45].',
     r'Typo checking and correction has an important impact on IME performance.'
    ],

    ['P14-1144',
     r'A major weakness of many existing scoring engines such as the Intelligent Essay Assessor™[13] is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer.',
     r'Recent work addresses this problem by scoring a particular dimension of essay quality such as coherence [16], technical errors, organization [18], and thesis clarity [19].',
     r'Our goal in this paper is to develop a computational model for scoring an essay along an under-investigated dimension — prompt adherence.',
     r'Prompt adherence refers to how related an essay’s content is to the prompt for which it was written.',
     r'Regarding task formulation, while Higgins et al. focus on classifying each sentence as having either good or bad adherence to the prompt, we focus on assigning a prompt adherence score to the entire essay, allowing the score to range from one to four points at half-point increments.',
     r'On the other hand, we employ a large variety of features, including lexical and knowledge-based features that encode how well the concepts in an essay match those in the prompt, LDA-based features that provide semantic generalizations of lexical features, and “error type” features that encode different types of errors the writer made that are related to prompt adherence.',
     r'First, we develop a scoring model for the prompt adherence dimension on student essays using a feature-rich approach.',
     r'Second, in order to stimulate further research on this task, we make our data set consisting of prompt adherence annotations of 830 essays publicly available.'
    ],

    ['P14-1145',
     r'We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses, as defined in WordNet.',
     r'A connotation lexicon, as introduced first by Feng et al. (2011), aims to encompass subtle shades of sentiment a word may conjure, even for seemingly objective words such as “sculpture”, “Ph.D.”, “rosettes”.',
     r'As a starting point, we study a more feasible task of learning the polarity of connotation.',
     r'However, for polysemous words, which correspond to most frequently used words, it would be an overly crude assumption that the same connotative polarity should be assigned for all senses of a given word.',
     r'the general overtone is slightly more negative with a touch of unpleasantness, or at least not as positive as that of the first sense.',
     r'End-users of such a lexicon may not wish to deal with Word Sense Disambiguation (WSD), which is known to be often too noisy to be incorporated into the pipeline with respect to other NLP tasks.',
     r'Therefore, in this work, we present the first unified approach that learns both sense- and word-level connotations simultaneously.',
     r'We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields (pairwise-MRF) and derive a loopy belief propagation algorithm for inference.',
     r'The key aspect of our approach is that we exploit the innate bipartite graph structure between words and senses encoded in WordNet.',
     r'Another contribution of our work is the introduction of loopy belief propagation (loopy-BP) as a lexicon induction algorithm.'
    ],

    ['P14-1146',
     r'The objective is to classify the sentiment polarity of a tweet as positive, negative or neutral.',
     r'The majority of existing approaches follow Pang et al. [33] and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.',
     r'It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering [4]. ',
     r'For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains [40, 47].',
     r'The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.',
     r'In this paper, we propose learning sentiment-specific word embedding (SSWE) for sentiment analysis.',
     r'We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.',
     r'We apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013.'
    ],

    ['P14-2004',
     r'This fact suggests that a dialog system should be also capable of conducting multi-topic conversations with users to provide them a more natural interaction with the system.',
     r'Although some multi-task dialog systems have been proposed [10, 6, 4], they have aimed at just choosing the most probable one for each input from the sub-systems, each of which is independently operated from others.',
     r'Thus, the text categorization approaches can only be effective for the user-initiative cases when users tend to mention the topic-related expressions explicitly in their utterances.',
     r'These knowledge-based methods have an advantage of dealing with system-initiative dialogs, because dialog flows can be controlled by the system based on given resources.',
     r'However, this aspect can limit the flexibility to handle the user’s responses which are contradictory to the system’s suggestions.',
     r'In this paper, we propose a composite kernel to explore various types of information obtained from Wikipedia for mixed-initiative dialog topic tracking without significant costs for building resources.',
     r'Our composite kernel consists of a history sequence and a domain context tree kernels, both of which are composed based on similar textual units in Wikipedia articles to a given dialog context.'
    ],

    ['P14-2005',
     r'Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity.',
     r'A critically important problem is how to measure the quality of a coreference resolution system.',
     r'The BLANC-gold metric was developed with the assumption that response mentions and key mentions are identical.',
     r'In reality, however, mentions need to be detected from natural language text and the result is, more often than not, imperfect: some key mentions may be missing in the response, and some response mentions may be spurious—so-called “twinless” mentions by Stoyanov et al. (2009).',
     r'Therefore, the identical-mention-set assumption limits BLANC-gold’s applicability when gold mentions are not available, or when one wants to have a single score measuring both the quality of mention detection and coreference resolution.',
     r'The goal of this paper is to extend the BLANC-gold metric to imperfect response mentions.',
     r'We first briefly review the original definition of BLANC, and rewrite its definition using set notation.',
     r'We then argue that the gold-mention assumption in Recasens and Hovy (2011) can be lifted without changing the original definition.'
    ],

    ['P14-2007',
     r'Thus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others.',
     r'With regard to this, we introduce a metric called sentiment annotation complexity (SAC).',
     r'The primary question is whether such complexity measurement is necessary at all.',
     r'To measure the “actual” time spent by an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration: the time for which the annotator has actually focused on the sentence during annotation. ',
     r'The novelty of our work is three-fold: (a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features.'
    ],

    ['P14-2008',
     r'Citations have been categorized and studied for a half-century [15] to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors [26]).',
     r'Early citation classification focused more on citation motivation [16], while later classification considered more the citation function [9].',
     r'One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations [3, 2, 1].1',
     r'We follow much of the recent work on citation classification and concentrate on citation polarity.'
    ],

    ['P14-2010',
     r'Unfortunately, labeling a large number of documents is a labor-intensive and time consuming process.',
     r'In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) [] which does not need labeled documents.',
     r'LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents.',
     r'An approach that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013).',
     r'We extend ClassifyLDA algorithm by “sprinkling” topics to unlabeled documents.',
     r'Sprinkling [] integrates class labels of documents into Latent Semantic Indexing (LSI)[].',
     r'The paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling.',
     r'As LDA uses higher order word associations [] while discovering topics, we hypothesize that sprinkling will improve text classification performance of ClassifyLDA.'
    ],

    ['P14-2012',
     r'The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships.',
     r'This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can perform well on new domains (the target domains).',
     r'The only study explicitly targeting this problem so far is by Plank and Moschitti [Plank and Moschitti2013] who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels.',
     r'It does not incorporate word cluster information at different levels of granularity.',
     r'It is unclear if this approach can encode real-valued features of words (such as word embeddings [Mnih and Hinton2007, Collobert and Weston2008]) effectively.',
     r'In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more naturally and effectively.',
     r'More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors.'
    ],

    ['P14-2013',
     r'However, references to entities in the real world are often ambiguous: there is a many-to-many relation between NE mentions and the entities they denote in the real world.',
     r'The NED task is to establish a correct mapping between each NE mention in a document and the real world entity it denotes.',
     r'Following most researchers in this area, we treat entries in a large knowledge base (KB) as surrogates for real world entities when carrying out NED and, in particular, use Wikipedia as the reference KB for disambiguating NE mentions.',
     r'The main hypothesis in this work is that different NEs in a document help to disambiguate each other.',
     r'In our approach we model each possible candidate for every NE mention in a document as a distinct node in a graph and model candidate coherence by links between the nodes.',
     r'The goal of disambiguation is to find a set of nodes where only one candidate is selected from the set of entities associated with each mention, e.g. a3, b2, c2.',
     r'Our approach first ranks all nodes in the solution graph using the Page-Rank algorithm, then re-ranks all nodes by combining the initial confidence and graph ranking scores.'
    ],

    ['P14-2016',
     r'This paper is an attempt to automate the detection of multilingual dictionaries on the web, through query construction for an arbitrary language pair.',
     r'Note that for the method to work, we require that the dictionary occurs in “list form”, that is it takes the form of a single document (or at least, a significant number of dictionary entries on a single page), and is not split across multiple small-scale sub-documents.'
    ],

    ['P14-2017',
     r'Cognates are words in different languages having the same etymology and a common ancestor.',
     r'Investigating pairs of cognates is very useful in historical and comparative linguistics, in the study of language relatedness [30], phylogenetic inference [1] and in identifying how and to what extent languages change over time.',
     r'In this paper, we propose a method for automatically determining pairs of cognates across languages.',
     r'The proposed method requires a list of known cognates and, for languages for which additional linguistic information is available, it can be customized to integrate historical information regarding the evolution of the language.'
    ],

    ['P14-2019',
     r'Although there exist some recent works to produce parallel corpora for Turkish-English pair, the produced corpus is only applicable for phrase-based training [22, 8].',
     r'A parallel treebank is a parallel corpus where the sentences in each language are syntactically (if necessary morphologically) annotated, and the sentences and words are aligned.',
     r'In this study, we report our preliminary efforts in constructing an English-Turkish parallel treebank corpus for statistical machine translation.',
     r'Our approach converts English parse trees into equivalent Turkish parse trees by applying several transformation heuristics.',
     r'The main components of our strategy are (i) tree permutation, where we permute the children of a node; and (ii) leaf replacement, where we replace English word token at a leaf node.'
    ],

    ['P14-2026',
     r'Reordering therefore becomes a key issue in SMT systems between distant language pairs.',
     r'Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective.',
     r'The purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system.',
     r'Experiment results showed that our pre-ordering rule set improved the BLEU score on the NIST 2006 evaluation data by 1.61.',
     r'Moreover, this rule set substantially decreased the total times of rule application about 60%, compared with a constituent-based approach (Wang et al., 2007).',
     r'By applying our rules and Wang et al.’s rules, one can use both dependency and constituency parsers for pre-ordering in Chinese-English PBSMT.',
     r'This is especially important on the point of the system combination of PBSMT systems, because the diversity of outputs from machine translation systems is important for system combination [2013].'
    ],

    ['P14-2029',
     r'Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions [19], articles [11], and collocations [7].',
     r'Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs [10, 14], such as the MT Quality Estimation Shared Tasks [2, §6], but relatively little on evaluating the grammaticality of naturally occurring text.',
     r'We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above.',
     r'We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality.'
    ],

    ['P14-2030',
     r'With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences [9, 19, 4, 22, 24].',
     r'While some users explicitly share their GPS coordinates through their Twitter clients, having a larger collection of automatically identified users within a region was preferable even though the predictions for any given user were uncertain.',
     r'Predicting social roles such as doctor, teacher, vegetarian, christian, may open the door to large-scale passive surveys of public discourse that dwarf what has been previously available to social scientists.',
     r'We present two studies showing that first-person social content (tweets) contains intuitive signals for such fine-grained roles.',
     r'We argue that non-trivial classifiers may be constructed based purely on leveraging simple linguistic patterns.'
    ],

    ['P14-2031',
     r'Wikipedia, as one of the most prominent collaboratively created resources, offers its users a platform to coordinate their writing, the so called talk or discussion pages [18].',
     r'In addition to that, Wikipedia stores all edits made to any of its pages in a revision history, which makes the actual writing process explicit.',
     r'We argue that linking these two resources helps to get a better picture of the collaborative writing process.',
     r'To enable such interaction, we extract segments from discussion pages, called turns, and connect them to corresponding edits in the respective article.',
     r'First, an automatic detection of corresponding edit-turn-pairs in Wikipedia pages might help users of the encyclopedia to better understand the development of the article they are reading.',
     r'Second, assuming that edits often introduce new knowledge to an article, it might be interesting to analyze how much of this knowledge was actually generated within the discourse on the discussion page.',
     r'First, we want to understand the nature of correspondence between Wikipedia article edits and discussion page turns.',
     r'Second, we want to know the distinctive properties of corresponding edit-turn-pairs and how to use these to automatically detect corresponding pairs.'
    ],

    ['P14-2032',
     r'Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing.',
     r'There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters [23, 19, 24, 20], and word-based, where the units are full words based on some dictionary or training lexicon [1, 25].',
     r'\newciteSun:2010:COLING details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new oov words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans.',
     r'In this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition.',
     r'Perhaps most importantly, this work presents a much more practical and usable form of classifier combination in the CWS context than existing methods offer.'
    ],

    ['P14-2033',
     r'It is well known that Chinese text does not come with natural word delimiters, and the first step for many Chinese language processing tasks is word segmentation, the automatic determination of word boundaries in Chinese text.',
     r'Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches.',
     r'This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques [22, 12].',
     r'The word segmentation accuracy on Chinese patents is very poor if the word segmentation model is trained on the Chinese TreeBank data, which consists of data sources from a variety of genres but no patents.',
     r'To address this issue, we annotated a corpus of 142 patents which contain about 440K words according to the Chinese TreeBank standards.',
     r'We trained a character-tagging based CRF model for word segmentation, and based on the writing style of patents, we propose a group of document-level features as well as a novel character part-of-speech feature (C_POS).'
    ],

    ['P14-2034',
     r'Segmentation is a common practice in Arabic NLP due to the language’s morphological richness.',
     r'However, the variety of Arabic dialects presents challenges in Arabic NLP.',
     r'Dialectal Arabic contains non-standard orthography, vocabulary, morphology, and syntax.',
     r'The model is an extension of the character-level conditional random field (CRF) model of Green and DeNero (2012).',
     r'We demonstrate that our system achieves better performance across the board, beating all three systems on MSA newswire, informal broadcast news, and Egyptian dialect.'
    ],

    ['P14-2035',
     r'The provision of compositionality in distributional models of meaning, where a word is represented as a vector of co-occurrence counts with every other word in the vocabulary, offers a solution to the fact that no text corpus, regardless of its size, is capable of providing reliable co-occurrence statistics for anything but very short text constituents.',
     r'An interesting question that has attracted the attention of researchers lately refers to the way in which these models affect ambiguous words; in other words, given a sentence such as “a man was waiting by the bank”, we are interested to know to what extent a composite vector can appropriately reflect the intended use of word ‘bank’ in that context, and how such a vector would differ, for example, from the vector of the sentence “a fisherman was waiting by the bank”.',
     r'Recent experimental evidence [] suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations.',
     r'Until now this idea has been verified on relatively simple compositional functions, usually involving some form of element-wise operation between the word vectors, such as addition or multiplication.',
     r'A potential explanation then for the effectiveness of the proposed prior disambiguation method can be sought on the limitations imposed by the compositional models under test.',
     r'The purpose of this paper is to investigate the hypothesis whether prior disambiguation is important in a pure tensor-based compositional model, where no simplifying assumptions have been made.'
    ],

    ['P14-2036',
     r'Treating microblogs as standard texts and directly classifying them cannot achieve the goal of effective classification because of sparseness problem.',
     r'Thus, external knowledge, such as news, provides rich supplementary information for analysing and mining microblogs.',
     r'Motivated by the idea of using topic model and external knowledge mentioned above, we present an LDA-based enriching method using the news corpus, and apply it to the task of microblog classification.',
     r'We formulate the topic inference problem for short texts as a convex optimization problem.',
     r'We enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective.',
     r'We evaluate our method on the real datasets and experiment results outperform the baseline methods.'
    ],

    ['P14-2039',
     r'Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television.',
     r'Automatic speech recognition (ASR) systems [], on the other hand, attempts to provide a cheap and fully automated solution to this problem.',
     r'However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone [].',
     r'An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type), instead of relying on trained workers [].',
     r'Workers type as much as they can of the input, and, while no one worker’s transcript is complete, the portions captured by various workers tend to overlap.',
     r'However, aligning these individual words in the correct sequential order remains a challenging problem.',
     r'In this paper, we introduce a novel sliding window technique which avoids the errors produced by previous systems at the boundaries of the chunks used for alignment.'
    ],

    ['P14-2040',
     r'As the volume of online documents has drastically increased, the analysis of topic bursts, topic drift or detection of topic is a practical problem attracting more and more attention [1, 23, 2, 13, 15, 9].',
     r'This paper proposes a method for detecting topic over time in series of documents.',
     r'We reinforced words related to a topic with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (LDA) [4] to these documents in order to extract topic candidates.',
     r'We examined our method by extrinsic evaluation, i.e., we applied the results of topic detection to extractive multi-document summarization.',
    ],

    ['P14-2044',
     r'The morphosyntactic function of words is reflected in two ways: their distributional properties, and their morphological structure.',
     r'These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features [9, 7].',
     r'But these features are difficult to combine because of their disparate representations.',
     r'In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Blei and Frazier, 2011).',
     r'The probability of word w1 following w2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w1 and w2, which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w1 and w2, which acts as a prior distribution on the induced clustering.'
    ],

    ['P14-2045',
     r'Most existing interfaces for syntactic search (querying over grammatical and syntactic structures) require structured query syntax.',
     r'However, most potential users do not have programming expertise, and are not likely to be at ease composing rigidly-structured queries.',
     r'However, we know of no prior work on how to display grammatical relations so that they can be easily recognized.',
     r'One current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [14]; we used this as the baseline presentation in our experiments because it employs the relation definitions found in the Stanford Dependency Parser’s manual [4].',
     r'Our results confirm that showing examples in the form of words or phrases significantly improves the accuracy with which grammatical relationships are recognized over the standard baseline of showing the relation name with blanks.',
     r'Our findings also showed that clausal relationships, which span longer distances in sentences, benefited significantly more from example phrases than either of the other treatments.',
     r'These findings suggest that a query interface in which a user enters a word of interest and the system shows candidate grammatical relations augmented with examples from the text will be more successful than the baseline of simply naming the relation and showing gaps where the participating words appear.'
    ],

    ['P14-2046',
     r'Since the speaker may not be able to pronounce the foreign-language orthography, phrasebooks additionally provide phonetic spellings that approximate the sounds of the foreign phrase.',
     r'These spellings employ the familiar writing system and sounds of the speaker’s language.',
     r'We take a statistical modeling approach to this problem, as is done in two lines of research that are most related.',
     r'The first is machine transliteration [3], in which names and technical terms are translated across languages with different sound systems.',
     r'The other is respelling generation [2], where an English speaker is given a phonetic hint about how to pronounce a rare or foreign word to another English speaker.'
    ],

    ['P14-2047',
     r'In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality.',
     r'The goal is to identify discourse processing tasks with high potential for improving translation systems.',
     r'Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to interpret multi-sentential text while statistical MT systems are trained to translate a single sentence in one language into a single sentence in another.',
     r'However, discourse devices are at play in the organization of information into complex sentences.',
     r'In our work, we quantify the relationship between information packaging, discourse devices, and translation quality.'
    ],

    ['P14-2048',
     r'First, we will show that using style-related linguistic features, such as frequencies of parts-of-speech n-grams and function words, it is possible to learn classifiers that distinguish machine-translated text from human-translated or native English text.',
     r'We will see that the success of such classifiers are strongly correlated with the quality of the underlying machine translation system.',
     r'In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU [].'
    ],

    ['P14-2049',
     r'A key assumption of most models of similarity is that a similarity relation is symmetric.',
     r'The symmetry assumption is not, however, universal, and it is not essential to all applications of similarity, especially when it comes to modeling human similarity judgments.',
     r'Thus, one source of asymmetry is the comparison of sparse and dense representations.',
     r'The relevance of such considerations to word similarity becomes clear when we consider that for many applications, word similarity measures need to be well-defined when comparing very frequent words with infrequent words.',
     r' Below we show that an asymmetric measure, using α and β biased in favor of the less frequent word, greatly improves the performance of a dependency-based vector model in capturing human similarity judgments.',
     r'In the following sections we present data showing that the performance of a dependency-based similarity system in capturing human similarity judgments can be greatly improved with rank-bias and α-skewing.'
    ],

    ['P14-2050',
     r'We thus seek a representation that captures semantic and syntactic similarities between words.',
     r'A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [16], stating that words in similar contexts have similar meanings.',
     r'Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community.',
     r'Among the state-of-the-art word-embedding methods is the skip-gram with negative sampling model (SkipGram), introduced by Mikolov et al. [21] and implemented in the word2vec software.',
     r'Previous work on neural word embeddings take the contexts of a word to be its linear context – words that precede and follow the target word, typically in a window of k tokens to each side.',
     r'In this work, we generalize the SkipGram model, and move from linear bag-of-words contexts to arbitrary word contexts.',
     r'Specifically, following work in sparse vector-space models [18, 24, 3], we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees.'
    ],

    ['P14-2051',
     r'Among these aspects of language use that are subject to diachronic change, this paper is concerned with the productivity of syntactic constructions, i.e., the range of lexical items with which a construction can be used.',
     r'More specifically, previous research points to a strong semantic component, in that the possibility of a novel use depends on how it semantically relates to prior usage.',
     r'The importance of semantics for syntactic productivity implies that the meaning of lexical items must be appropriately taken into account when studying the distribution of constructions, which calls for an empirical operationalization of semantics.',
     r'In this paper, I present a third alternative that takes advantage of advances in computational linguistics and draws on a distributionally-based measure of semantic similarity.'
    ],

    ['P14-2052',
     r'Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary.',
     r'Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization [16, 8, 20].',
     r'It is important for generated summaries to have a discourse structure that is similar to that of the source document.',
     r'We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser.'
     r'First, we represent a document as a nested tree, which is composed of two types of tree structures: a document tree and a sentence tree.',
     r'Finally, we formulate the problem of single document summarization as that of combinatorial optimization, which is based on the trimming of the nested tree.'
    ],

    ['P14-2053',
     r'The aim of this work is to automatically generate questions for such pedagogical purposes.'
    ],

    ['P14-2054',
     r'Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time.',
     r'They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case.',
     r'In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming.',
     r'Our method extends Eisner’s cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted words and the rightmost kept word within the span, resulting in O⁢(n6) time complexity and O⁢(n4) space complexity.',
     r'We further propose a faster approximate algorithm based on Lagrangian relaxation, which has T⁢O⁢(n4) running time and O⁢(n3) space complexity (T is the iteration number in the subgradient decent algorithm).',
     r'Experiments on the popular Edinburgh dataset show that the proposed approach is 10 times faster than a high-performance commercial ILP solver.'
    ],

    ['P14-2055',
     r'Understandably in automatic summarization as well, it is useful to keep a background set of documents to represent general facts and their frequency in the domain.',
     r'In this work, we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents.',
     r'Bayesian surprise is the difference between the prior and posterior distributions over the hypotheses which quantifies the extent to which the new data (the news report) has changed a user’s prior beliefs about the world.',
     r'In this work, we exemplify how Bayesian surprise can be used to do content selection for text summarization.',
     r'Here a user’s prior knowledge is approximated by a background corpus and we show how to identify sentences from the input set which are most surprising with respect to this background.',
     r' We use the method to do two types of summarization tasks: a) generic news summarization which uses a large random collection of news articles as the background, and b) update summarization where the background is a smaller but specific set of news documents on the same topic as the input set.'
    ],

    ['P14-2058',
     r'The keyword method is a mnemonic device [] that is especially suitable for vocabulary acquisition in second language learning [].',
     r'In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s).',
     r'In this paper, we overcome these limitations by introducing a semi-automatic system implementing the keyword method that builds upon the keyword selection mechanism of and combines it with a state-of-the-art creative sentence generation framework [].',
     r'We set up an experiment to simulate the situation in which a teacher needs to prepare material for a vocabulary teaching resource'
    ],

    ['P14-2059',
     r'So we define a context-based citation recommendation (cbcr) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft.',
     r'However, our current paper has more modest aims: we present initial results using existing IR-based approaches and we introduce an evaluation method and metric.',
     r'A main problem we face is that evaluating the performance of these systems ultimately requires human judgement',
     r'Fortunately there is already an abundance of data that meets our requirements: every scientific paper contains human “judgements” in the form of citations to other papers which are contextually appropriate: that is, relevant to specific passages of the document and aligned with its argumentative structure.'
    ],

    ['P14-2060',
     r'The basic task of text normalization is to convert non-standard words (NSWs) — numbers, abbreviations, dates, etc. — into standard words, though depending on the task and the domain a greater or lesser number of these NSWs may need to be normalized.',
     r'Which normalizations are required depends very much on the application.',
     r'What is also very application-dependent is the cost of errors in normalization.',
     r'In this paper we concentrate on abbreviations, which we define as alphabetic NSWs that it would be normal to pronounce as their expansion.',
     r'n important principle in text normalization for TTS is do no harm.',
     r'We present methods for learning abbreviation expansion models that favor high precision (incorrect expansions <2⁢%).'
    ],

    ['P14-2061',
     r'Prior research on language identification is heavily biased towards written and spoken languages [8, 27, 14, 18].',
     r'While language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages.',
     r'Sign language recognition is the recognition of the meaning of the signs in a given known sign language, whereas sign language identification is the recognition of the sign language itself from given signs. [20, 21, 9, 6], very little research exists on sign language identification except for the work by [10], where it is shown that sign language identification can be done using linguistically motivated features. ',
     r'Our contributions: a) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages.',
     r'b) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length).'
    ],

    ['P14-2062',
     r'\newcitesnow2008 showed, however, that crowdsourced annotations can produce similar results to annotations made by experts.',
     r'However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica []. ',
     r'Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations.',
     r'In this paper, we investigate how well lay annotators can produce POS labels for Twitter data.',
     r'In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side).',
     r'In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE [].',
     r'We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks.',
     r'We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica [].'
    ],

    ['P14-2063',
     r'Although several well-regarded sentiment lexicons are available in English [9, 17], the same is not true for most of the world’s languages.',
     r'In this paper, we strive to produce a comprehensive set of sentiment lexicons for the worlds’ major languages.',
     r'New Sentiment Analysis Resources – We have generated sentiment lexicons for 136 major languages via graph propagation which are now publicly ',
     r'Large-Scale Language Knowledge Graph Analysis – We have created a massive comprehensive knowledge graph of 7 million vocabulary words from 136 languages with over 131 million semantic inter-language links, which proves valuable when doing alignment between definitions in different languages.',
     r'Extrinsic Evaluation – We elucidate the sentiment consistency of entities reported in different language editions of Wikipedia using our propagated lexicons.'
    ],

    ['P14-2065',
     r'Verbs vary in terms of which syntactic frames they can appear in (Table 1).',
     r'However, most theorists posit that there is a systematic relationship between the semantics of a verb and the syntactic frames in which it can appear [9].',
     r'Note that this is a simplification in that there are non-causal verbs that appear in both the NP V NP frame and the NP V frame.'
    ],

    ['P14-2066',
     r'Since the strength and scope of an argument can be a crucial factor in its success, it is important to understand the effects of statement strength in communication.',
     r'A first step towards addressing this question is to be able to distinguish between strong and weak statements. ',
     r'An intriguing observation is that many researchers submit multiple versions of the same paper on arXiv.',
     r'The main contribution of this work is to provide the first large-scale corpus of sentence-level revisions for studying a broad range of variations in statement strength.'
    ],

    ['P14-2068',
     r'However, less is known about this phenomenon in social media — a domain whose endemic uncertainty makes proper treatment of factuality even more crucial [10].',
     r'Successful automation of factuality judgments could help to detect online rumors [15], and might enable new applications, such as the computation of reliability ratings for ongoing stories.',
     r'This paper investigates how linguistic resources and extra-linguistic factors affect perceptions of the certainty of quoted information in Twitter.',
     r'We explore the specific linguistic feature that affect factuality judgments, and compare our findings with previously-proposed groupings of factuality-related predicates.'
    ],

    ['P14-2069',
     r'Usually, a high quality emotion lexicon play a significant role when apply the unsupervised approaches for fine-grained emotion classification.',
     r'However, since a specific word can carry various emotions in different domains, a general-purpose emotion lexicon is less accurate and less informative than a domain-specific lexicon [1].',
     r'To meet the challenges mentioned above, we propose a novel EaLDA model to construct a domain-specific emotion lexicon consisting of six primary emotions (i.e., anger, disgust, fear, joy, sadness and surprise).',
     r'The proposed EaLDA model extends the standard Latent Dirichlet Allocation (LDA) [3] model by employing a small set of seeds to guide the model generating topics.',
     r' Our approach is a weakly supervised approach since only some seeds emotion sentiment words are needed to lanch the process of lexicon construction.'
    ],

    ['P14-2070',
     r'Many approaches to sentiment analysis make use of lexical resources – i.e. lists of positive and negative words – often deployed as baselines or as features for other methods, usually machine learning based [].',
     r'The quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons.',
     r'In this work, we aim at automatically producing a high coverage and high precision emotion lexicon using distributional semantics, with numerical scores associated with each emotion, like it has already been done for sentiment analysis.',
     r'We also evaluate our lexicon by integrating it in unsupervised classification and regression settings for emotion recognition.'
    ],

    ['P14-2071',
     r'In this paper, we focus on sentiment analysis of Twitter data (tweets).',
     r'However, all of these features only capture local information in the data and do not take into account of the global higher-level information, such as topic information.',
     r'We first propose a universal sentiment model that utilizes various features and resources.',
     r'We introduce a topic-based mixture model for Twitter sentiment.',
     r'We propose a smoothing technique through interpolation between universal model and topic-based mixture model.',
     r'We also compare different approaches for topic modeling, such as cross-domain topic identification by utilizing data from newswire domain.'
    ],

    ['P14-2073',
     r'We extend a popular model, latent Dirichlet allocation (LDA), to unbounded streams of documents.',
     r'Canini et al. (2009) presented a method for LDA inference based on particle filters, where a sample set of models is updated online with each new token observed from a stream.',
     r'This algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense [6].',
     r'In the current work we propose using reservoir sampling in the rejuvenation step to reduce the storage complexity of the particle filter to O⁢(1).',
    ],

    ['P14-2074',
     r'State of the art visual detectors have made it possible to hypothesise what is in an image [6, 5], paving the way for automatic image description systems.',
     r'The aim of such systems is to extract and reason about visual aspects of images to generate a human-like description.',
     r'In this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets.'
    ],

    ['P14-2078',
     r'The Entity Linking (EL) task consists in linking name mentions of named entities (NEs) found in a document to their corresponding entities in a reference Knowledge Base (KB).',
     r'Dealing with ambiguity is one of the key difficulties in this task, since mentions are often highly polysemous, and potentially related to many different KB entries.',
     r'The NED problem is related to the Word Sense Disambiguation (WSD) problem [16], and is often more challenging since mentions of NEs can be highly ambiguous.',
     r'The paper makes the following novel propositions: 1) the ontology used to evaluate the relatedness of candidates is replaced by internal links and categories from the Wikipedia corpus; 2) the coherence of entities is improved prior to the calculation of semantic relatedness using a co-reference resolution algorithm, and a NE label correction method; 3) the proposed method is robust enough to improve the performance of existing entity linking annotation engines, which are capable of providing a set of ranked candidates for each annotation in a document.'
    ],

    ['P14-2080',
     r'Cross-Language Information Retrieval (CLIR) for the domain of web search successfully leverages state-of-the-art Statistical Machine Translation (SMT) to either produce a single most probable translation, or a weighted list of alternatives, that is used as search query to a standard search engine [5, 25].',
     r'Parallel data for translation have to be extracted with some effort from comparable or noisy parallel data [26, 22], however, relevance judgments are often straightforwardly encoded in special domains.',
     r'Besides a rich citation structure, patent documents and Wikipedia articles contain a number of further cues on relatedness that can be exploited as features in learning-to-rank approaches.',
     r'The main contribution of this paper is a thorough evaluation of dense and sparse features for learning-to-rank that have so far been used only monolingually or only on either patents or Wikipedia.'
     r'We show that for both domains, patents and Wikipedia, jointly learning bilingual sparse word associations and dense knowledge-based similarities directly on relevance ranked data improves significantly over approaches that use either only sparse or only dense features, and over approaches that combine query translation by SMT with standard retrieval in the target language.'
    ],

    ['P14-2082',
     r'The TimeBank Corpus [6] helped usher in a wave of event ordering research with data-driven algorithms.',
     r'It provided for a common dataset of annotations between events and time expressions that allowed the community to compare approaches.',
     r'This paper is the first attempt to annotate a document’s entire temporal graph.',
     r'A consequence of focusing on all relations is a shift from the traditional temporal relation classification task, where the system is given a pair of events and asked only to label the type of temporal relation, to a temporal relation identification task, where the system must determine for itself which events in the document to pair up.',
     r'We describe the first annotation framework that forces annotators to annotate all pairs11As discussed below, all pairs in a given window size.'
    ],

    ['P14-2083',
     r'In NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory.',
     r'However, it is well known that there are linguistically hard cases [], where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions.',
     r' Instead of using overly specific annotation guidelines, designed to minimize inter-annotator disagreement [], and adjudicating between annotators of different opinions, we should embrace systematic inter-annotator disagreements.',
     r'To substantiate our claims, we first compare the distribution of inter-annotator disagreements across domains and languages, showing that most disagreements are systematic (Section 2).',
     r'We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors, and which ones reflect linguistically hard cases (Section 3).',
     r'Finally, in Section 4, we present an experiment trying to learn a model to distinguish between hard cases and annotation errors.'
    ],

    ['P14-2084',
     r'This work concerns the task of detecting verbal irony online.',
     r'These approaches have achieved some success, but necessarily face an upper-bound: the exact same sentence can be both intended ironically and unironically, depending on the context (including the speaker and the topic at hand).',
     r'We introduce the first version of the reddit irony corpus, composed of annotated comments from the social news website reddit.',
     r'We provide empirical evidence that human annotators consistently rely on contextual information to make ironic/unironic sentence judgements.',
     r'We show that the standard ‘bag-of-words’ approach to text classification fails to accurately judge ironic intent on those cases for which humans required additional context.'
    ],

    ['P14-2085',
     r'In this work, we focus on the automatic prediction of whether a verb in context is used in a stative or in a dynamic sense, the most fundamental distinction in all taxonomies of aspectual class.',
     r'Following Siegel and McKeown (2000), we aim to automatically classify clauses for fundamental aspectual class, a function of the main verb and a select group of complements, which may differ per verb [26, 28]. ',
     r'Instead we predict the aspectual class of verbs in the context of their arguments and modifiers.',
     r'In addition, we show that type-based features, including novel distributional features based on representative verbs, accurately predict predominant aspectual class for unseen verb types.',
     r'Our work differs from prior work in that we treat the problem as a three-way classification task, predicting dynamic, stative or both as the aspectual class of a verb in context.'
    ],

    ['P14-2087',
     r'To disambiguate a homonymous word in a given context, proposed a method that measured the degree of overlap between the glosses of the target and context words.',
     r'Compounding this problem is the fact that many Lesk variants limited the concept of overlap to the literal interpretation of string matching (with their own variants such as length-sensitive matching (), etc.), and it was not until recently that overlap started to take on other forms such as tree-matching () and vector space models ().',
     r'To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD.'
    ],

    ['P14-2088',
     r'While a number of different approaches for domain adaptation have been proposed [21, 26], they tend to emphasize bag-of-words features for classification tasks such as sentiment analysis.',
     r'Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [25].',
     r'In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. ',
     r'To exploit this structure, we propose two alternative noising techniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template.',
     r'As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts.'
    ],

    ['P14-2092',
     r'Current Statistical Machine Translation (SMT) approaches model the translation problem as a process of generating a derivation of atomic translation units, assuming that every unit is drawn out of the same model. ',
     r'Despite good results in many tasks, such a method ignores the roles of each source word and is somewhat different from the way used by translators.',
     r'For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts.',
     r'We develop a skeleton-based model which divides translation into two sub-models: a skeleton translation model (i.e., translating the key elements) and a full translation model (i.e., translating the remaining source words and generating the complete translation).',
     r'We develop a skeletal language model to describe the possibility of translation skeleton and handle some of the long-distance word dependencies.',
     r'We apply the proposed model to Chinese-English phrase-based MT and demonstrate promising BLEU improvements and TER reductions on the NIST evaluation data.'
    ],

    ['P14-2095',
     r'Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages.',
     r'Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations.',
     r'Annotation projection, on the other hand, does not require any changes to the feature representation.',
     r'Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one.',
     r'The approach proposed here, which we will refer to as feature representation projection (FRP), constitutes an alternative to direct model transfer and annotation projection and can be seen as a compromise between the two.',
     r'Instead of designing this representation manually, however, we create compact monolingual feature representations for source and target languages separately and automatically estimate the mapping between the two from parallel data.',
     r'Compared to annotation projection, our approach may be expected to be less sensitive to parallel data quality, since we do not have to commit to a particular prediction on a given instance from parallel data.'
    ],

    ['P14-2096',
     r'Several works have focused on creating cross-language links between Wikipedia language versions [7, 9] or finding a cross-language link for each entity mention in a Wikipedia article, namely Cross-Language Link Discovery (CLLD) [10, 5].',
     r'However, when linking between different online encyclopedia platforms this is more difficult as many of these structural features are different or not shared.',
     r'Other methods must be used along with title translation to create a more robust linking tool.',
     r'In this paper, we propose a method comprising title and hypernym translation and mixed-language topic model methods to select and link related articles between the English Wikipedia and Baidu Baike online encyclopedias.'
    ],

    ['P14-2098',
     r'One of the tutors’ tasks is to isolate writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected.',
     r'We propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence.',
     r'The contributions of this paper are: (1) We show empirically that a major challenge in correction detection is to determine the number of edits that address the same error.',
     r'(2) We have developed a merging model that reduces mis-detection by 1/3, leading to significant improvement in the accuracies of combined correction detection and error type selection.',
     r'(3) We have conducted experiments across multiple corpora, indicating that the proposed merging model is generalizable.'
    ],

    ['P14-2099',
     r'Past applications of NLP have sought to parse privacy policies into machine-readable representations [5] or extract sub-policies from larger documents [14].',
     r'This paper instead analyzes policies in aggregate, seeking to align sections of policies.',
     r'A new corpus of over 1,000 privacy policies gathered from widely used websites, manually segmented into subtitled sections by crowdworkers (§2).',
     r'An unsupervised approach to aligning the policy sections based on the issues they discuss.',
     r'Two reusable evaluation benchmarks for the resulting alignment of policy sections (§4).'
    ],

    ['P14-2101',
     r'Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic.',
     r'As opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counter part on existing external sources',
     r'We propose a novel approach for topics labelling that relies on term relevance of documents relating to a topic; and',
     r'We show that summarisation algorithms, which are independent of extenal sources, can be used with success to label topics, presenting a higher perfomance than the top-n terms baseline.'
    ],

    ['P14-2103',
     r'But interpreting such lists is not always straightforward, particularly since background knowledge may be required[5].',
     r'Textual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically [17, 15, 14].',
     r'Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia.',
     r'This paper introduces an alternative graph-based approach which is unsupervised and less computationally intensive than Lau et al. (2011).'
    ],

    ['P14-2104',
     r'Semantic Role Labeling (SRL) is the task of automatically annotating the predicate-argument structure in a sentence with semantic roles.',
     r'Agglutinative languages such as Japanese, Korean, and Turkish are computationally difficult due to word-form sparsity, variable word order, and the challenge of using rich morphological features.',
     r'In this paper, we describe a Korean SRL system which achieves 81% labeled semantic F1-score.',
     r'Two factors proved crucial in the performance of our SRL system: (i) The analysis of fine-grained morphological tags specific to Korean, and (ii) the use of latent stem and morpheme representations to deal with sparsity.',
     r'Besides the contribution of this model and SRL system, we also report on the creation and availability of a new semantically annotated Korean corpus, covering over 8,000 sentences.'
    ],

    ['P14-2105',
     r'We assumed such questions are answerable by issuing a single-relation query that consists of the relation and an argument entity, against a knowledge base (KB).',
     r'At the core of our approach is a novel semantic similarity model using convolutional neural networks.',
     r' Leveraging the question paraphrase data mined from the WikiAnswers corpus by Fader et al. (2013), we train two semantic similarity models: one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation.',
     r'The answer to the question can thus be derived by finding the relation–entity triple r⁢(e1,e2) in the KB and returning the entity not mentioned in the question.'
    ],

    ['P14-2106',
     r'In this work, we will investigate the effect of semantic information using predicted POS tags.',
     r'We will test three different parsers representative of successful paradigms in dependency parsing.',
     r'We will also examine the LTH conversion, with richer structure and an extended set of dependency labels.',
     r'For the sake of comparison, we will also perform the experiments using syntactic/semantic clusters automatically acquired from corpora.',
     r'We will run parser combination experiments with and without semantic information, to determine whether it is useful in the combined parsers.'
    ],

    ['P14-2108',
     r'We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) [], showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) [].',
     r'We discuss some of the differences in annotation style and source material that make a direct comparison problematic.',
     r'Some first steps at analysis of the parsing results indicate aspects of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.',
     r'The PPCMBE is a million-word treebank created for researching changes in English syntax.'
    ],

    ['P14-2109',
     r'Phrase-structure parsing is usually evaluated using evalb [], which provides a score based on matching brackets.',
     r'While this metric serves a valuable purpose in pushing parser research forward, it has limited utility for understanding what sorts of errors a parser is making.',
     r'First, inspired by the tradition of Tree Adjoining Grammar-based research [], we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in .',
     r'Second, we use a set of regular expressions (henceforth “regexes”) that categorize the possible structures in the treebank.',
     r'The crucial step is that we integrate these regexes into the spinal etrees.'
    ],

    ['P14-2110',
     r'Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language.',
     r'Training of polylingual topic models requires parallel or comparable corpora: document tuples from multiple languages that discuss the same topic.',
     r'The result: an inability to train polylingual models on social media.',
     r'Social media is filled with examples of code-switching, where users switch between two or more languages, both in a conversation and even a single message [].',
     r'We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics.'
    ],

    ['P14-2113',
     r'In this work, we investigate the heretofore novel task of dispute detection in online discussions.',
     r'In contrast, we investigate methods for the automatic detection, i.e. prediction, of discussions involving disputes.',
     r'As a result, we propose a sentiment analysis approach for online dispute detection that identifies the sequence of sentence-level sentiments (i.e. very negative, negative, neutral, positive, very positive) expressed during the discussion and uses them as features in a classifier that predicts the dispute/non-dispute label for the discussion as a whole.',
     r'We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 non-disputes).'
    ],

    ['P14-2114',
     r'Event extraction is to automatically identify events from text with information about what happened, when, where, to whom, and why.',
     r'Approaches for event extraction include knowledge-based [12, 15], data-driven [11] and a combination of the above two categories [5].',
     r'Social media messages are often short and evolve rapidly over time.',
     r'We can treat an event as a latent variable and model the generation of an event as a joint distribution of its individual event elements.',
     r'We thus propose a Latent Event Model (LEM) which can automatically detect events from social media without the use of labeled data.'
    ],

    ['P14-2118',
     r'Concreteness, the degree to which language has a perceptible physical referent, and subjectivity, the extent to which linguistic meaning depends on the perspective of the speaker, are well established cognitive and linguistic notions.',
     r'However, recent work has highlighted the application of subjectivity analysis to lexical semantics, for instance to the tasks of disambiguating words according to their usage or sense [28, 2].',
     r'The importance of concreteness to NLP systems is likely to grow with the emergence of multi-modal semantic models [8, 25].',
     r'In this paper, we show how concreteness and subjectivity can be applied together to produce performance improvements on two classification problems: distinguishing literal and non-literal adjective-noun pairs [27], and classifying the modification type exhibited by such pairs [5].'
    ],

    ['P14-2119',
     r'Relation extraction is the task of tagging semantic relations between pairs of entities from free text.',
     r'One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data []. ',
     r'In this paper, we present the first effective approach, 𝐆⁢𝗎𝗂𝖽𝖾𝖽⁢𝖣𝖲 (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences.',
     r'To demonstrate the effectiveness of our proposed approach, we extend 𝖬𝖨𝖬𝖫 [], a state-of-the-art distant supervision model and show a significant improvement of 13.5% in F-score on the relation extraction benchmark TAC-KBP [] dataset.'
    ],

    ['P14-2120',
     r'This paper addresses a typical sub-task in textual inference scenarios, of recognizing implied predicate-argument relationships which are not expressed explicitly through syntactic structure.',
     r'Particularly, we investigate the setting of Recognizing Textual Entailment (RTE) as a typical scenario of textual inference.',
     r'An RTE problem instance is composed of two text fragments, termed Text and Hypothesis, as input.',
     r'A common approach for recognizing textual entailment is to verify that all the textual elements of the Hypothesis are covered, or aligned, by elements of the Text.',
     r'Two terms in the Hypothesis, predicate and argument, are marked, where a predicate-argument relationship between them is explicit in the Hypothesis syntactic structure.',
     r'The task is to recognize whether the predicate-argument relationship, as expressed in the Hypothesis, holds implicitly also in the Text.'
    ],

    ['P14-2121',
     r'This means that metaphorically used words not only have very different interpretations than literally used words, but they are also common enough to pose a significant challenge for computational linguistics.',
     r'Starting with Wilks (1978), the problem of metaphor has been approached as an identification task: first identify or detect metaphoric expressions and then (1) prevent them from interfering with computational treatments of literal expressions and (2) use them to gain additional insight about a text (e.g., Carbonell, 1980; Neuman & Nave, 2009).',
     r'The identification or detection task has been approached as a binary classification problem: for a given unit of language (e.g., word, phrase, sentence) decide whether it is metaphoric or non-metaphoric.',
     r'This binary distinction assumes a clear boundary between the two; in other words, it assumes that metaphoricity is a discrete property.',
     r'However, three strands of theoretical research show that metaphoricity is not a discrete property.',
     r' The goal is to produce a computationally derived measurement that models the gradient nature of metaphoricity, with the result that metaphors which are clearly and consciously seen as metaphors score closer to 1 and metaphors which are not realized by speakers to be metaphoric score further from 1. '
    ],

    ['P14-2122',
     r'Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT.',
     r'Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used [], yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety.',
     r'In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries [], relies on statistical criteria instead of manually crafted standards.',
     r'This paper is dedicated to bilingual UWS on large-scale corpora to support SMT.',
     r'To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training.'
    ],

    ['P14-2127',
    r'What makes large-scale MT training so hard then?',
    r'After numerous attempts by various researchers [17, 20, 1, 2, 5, 9, 10], the recent work of \newciteyu+:2013 finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning.',
    r'We generalize the latent variable violation-fixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottom-up parsing as special cases (see Fig. 1).',
    r'We show that syntax-based MT, with its better handling of long-distance reordering, can exploit a larger portion of the training set, which facilitates sparse lexicalized features.',
    r'Experiments show that our training algorithm outperforms mainstream tuning methods (which optimize on small devsets) by +1.2 Bleuover Mertand Proon FBIS.'
    ],

    ['P14-2128',
     r'The task of dependency parsing is to identify the lexical head of each of the tokens in a string.',
     r'However, there are a number of reasons that it is not necessary to parse punctuations:',
     r'In this method, punctuations are not associated with lexical heads, but are treated as properties of their neighbouring words.',
     r'In this work, we report results on an arc-standard transition-based parser.'
    ],

    ['P14-2129',
     r'Parsing full hierarchical syntactic structures is costly, and some NLP applications that could benefit from parses instead substitute shallow proxies such as NP chunks.',
     r'Still, these partial annotations omit all but the most basic syntactic segmentation, ignoring the abundant local structure that could be of utility even in the absence of fully connected structures.',
     r'One way to provide local hierarchical syntactic structures without fully connected trees is to focus on providing full hierarchical annotations for structures within a local window, ignoring global constituents outside that window.',
     r'In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L.',
     r'We pursue this topic via tree transformation, whereby non-root non-terminals labeling constituents of span >L in the tree are recursively elided and their children promoted to attach to their parent.'
     r'In this paper, we propose several methods to parse hedge constituents and examine their accuracy/efficiency tradeoffs.'
    ],

    ['P14-2133',
     r'This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space.',
     r'In order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations.',
     r'The fundamental question we want to explore is whether embeddings provide any information beyond what a conventional parser is able to induce from labeled parse trees.',
     r'The fact that word embedding features result in nontrivial gains for discriminative dependency parsing [2], but do not appear to be effective for constituency parsing, points to an interesting structural difference between the two tasks.',
     r'We hypothesize that dependency parsers benefit from the introduction of features (like clusters and embeddings) that provide syntactic abstractions; but that constituency parsers already have access to such abstractions in the form of supervised preterminal tags.'
    ],

    ['P14-2134',
     r'The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time.',
     r'In short: language is situated.',
     r'In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography.',
     r'In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language.',
     r'Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user [], our primary input is textual data, supplemented with metadata about the author and the moment of authorship.'
    ],

    ['P14-2135',
     r'Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system [3].',
     r'Such models extract information about the perceptible characteristics of words from data collected in property norming experiments [22, 24] or directly from ‘raw’ data sources such as images [11, 6].',
     r'Existing multi-modal architectures generally extract and process all the information from their specified sources of perceptual input.',
     r'Despite these results, the advantage of multi-modal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity.',
     r'In light of these considerations, we propose a novel algorithm for approximating conceptual concreteness.'
    ],

    ['P14-2137',
     r'In this paper, we illustrate such importance using named entity (NE) translation mining problem.',
     r'To address this challenge, current state-of-the-art approaches build an entity graph for each language corpus, and align the two graphs by propagating the seed translation similarities (Figure 1)',
     r'When two graphs are obtained from parallel corpora, graphs are symmetric and “blind propagation” described above is effective.',
     r'In contrast, we propose to explore corpus latent features (LF), to complement the sparsity problem of EF: Out of 158 randomly chosen correct relation translation pairs we labeled, 64% has only one co-occurring entity pair, which makes EF not very effective to identify these relation translations.',
     r'We observe the complementary nature of EF and LF, then propose a hybrid approach combining both features.'
    ],
    ['P14-2138',
     r'The task of Native Language Identification (NLI) is to determine the first language of the writer of a text in another language.',
     r'In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy.',
     r'Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken.',
     r'We design an algorithm to identify the most discriminative words and the character bigrams that are indicative of such words, and perform two experiments to quantify their impact on the NLI task.'
     r'We conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1.'
    ]
]
def main():
    print intro_sent

if __name__ == '__main__':
    main()
